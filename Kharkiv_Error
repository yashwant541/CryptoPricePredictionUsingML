import streamlit as st
import numpy as np
np.int = int
np.float = float

import math
import nltk
import re
import csv
import sys
import os
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import string
from dateutil.parser import parse as parse_date
from fuzzywuzzy import fuzz

# Setup NLTK path (optional: update as needed)
nltk.data.path.append(r'C:\Users\2011747\nltk_library')

# Weighted keyword lists
APPROVER_KEYWORDS = {
    "approved": 100, "granted": 95, "confirmed": 90, "accepted": 90,
    "authorized": 95, "approve": 80, "confirm": 80, "accept": 80,
    "approval": 50, "clearance": 70, "endorsed": 75, "signed off": 85,
    "cleared": 80, "validated": 85, "ratified": 90, "sanctioned": 90
}

REQUESTER_KEYWORDS = {
    "request": 100, "require": 95, "seek approval": 90, "need approval": 90,
    "asking": 80, "petition": 75, "approval": 30, "pending": 60,
    "remind": 70, "follow up": 65, "submitted": 85, "application": 80,
    "awaiting": 75, "petition": 70, "seeking": 85
}

PRONOUN_PATTERNS = [
    (r"\byou\b.*\bapprove\b", "approver", 50),
    (r"\byour\b.*\bapproval\b", "approver", 40),
    (r"\bplease\b.*\bapprove\b", "approver", 60),
    (r"\bi\b.*\brequest\b", "requester", 55),
    (r"\bwe\b.*\brequest\b", "requester", 50),
    (r"\bplease\b.*\breview\b", "approver", 40),
    (r"\bkindly\b.*\bapprove\b", "approver", 70)
]

# Enhanced Temporal Weighting Functions
def sigmoid(x, steepness=10, midpoint=0.5):
    """Smooth S-curve transition between requester and approver weights"""
    return 1 / (1 + math.exp(-steepness * (x - midpoint)))

def logarithmic_weight(position, total_emails):
    """More sensitive to early positions in thread"""
    if total_emails <= 1:
        return 0.5
    normalized_pos = position / (total_emails - 1)
    return 1 - math.log(1 + (1 - normalized_pos) * 9, 10)

def hybrid_temporal_weight(position, total_emails, sigmoid_ratio=0.7):
    """Combine both sigmoid and logarithmic weights"""
    if total_emails <= 1:
        return (0.5, 0.5)
    
    normalized_pos = position / (total_emails - 1)
    req_sig, app_sig = (1 - sigmoid(normalized_pos)), sigmoid(normalized_pos)
    log_weight = logarithmic_weight(position, total_emails)
    
    # Blend them
    requester_weight = sigmoid_ratio * (1 - log_weight) + (1 - sigmoid_ratio) * req_sig
    approver_weight = sigmoid_ratio * log_weight + (1 - sigmoid_ratio) * app_sig
    
    # Normalize
    total = requester_weight + approver_weight
    return (requester_weight/total, approver_weight/total)

def time_decay_weight(email_datetime, thread_start, thread_end, half_life_hours=24):
    """Weight based on absolute time differences"""
    total_span = (thread_end - thread_start).total_seconds()
    if total_span <= 0:
        return (0.5, 0.5)
    
    elapsed = (email_datetime - thread_start).total_seconds()
    decay_factor = 0.5 ** (elapsed / (half_life_hours * 3600))
    return (decay_factor, 1 - decay_factor)

def split_emails(raw_text):
    parts = re.split(r"(?=^From: )", raw_text, flags=re.IGNORECASE | re.MULTILINE)
    if parts and not parts[0].strip().lower().startswith("from:"):
        first = parts.pop(0)
        parts = [first] + parts
    return parts

def extract_field(email, field):
    pattern = rf"{field}:(.*)"
    match = re.search(pattern, email, re.IGNORECASE)
    return match.group(1).strip() if match else ""

def parse_date_time(date_str):
    if not date_str:
        return None
    try:
        return datetime.strptime(date_str.strip(), "%A, %B %d, %Y %I:%M %p")
    except Exception:
        return None

def extract_body(email):
    split_point = re.search(r"\n\s*\n", email)
    return email[split_point.end():].strip() if split_point else ""

def clean_participant_name(raw_name):
    """Extract clean name from potentially messy strings, removing all email addresses"""
    if not raw_name:
        return ""
    
    # Remove email addresses (anything in angle brackets)
    cleaned = re.sub(r'<[^>]+>', '', raw_name)
    # Remove any remaining special characters except spaces and commas
    cleaned = re.sub(r'[;"\']', '', cleaned)
    # Normalize whitespace and remove leading/trailing spaces
    cleaned = ' '.join(cleaned.split()).strip()
    # Remove trailing commas
    cleaned = cleaned.rstrip(',')
    return cleaned

def find_matching_statement(email_body, keywords, threshold=80):
    sentences = re.split(r'(?<=[.!?])\s+', email_body.strip())
    exact_matches = []
    fuzzy_matches = []

    for sentence in sentences:
        clean_sentence = sentence.lower()
        for kw in keywords:
            if kw in clean_sentence:
                exact_matches.append(sentence.strip())
            else:
                ratio = fuzz.partial_ratio(kw, clean_sentence)
                if ratio >= threshold:
                    fuzzy_matches.append((sentence.strip(), kw, ratio))

    if exact_matches:
        return "; ".join(exact_matches), "exact"
    elif fuzzy_matches:
        best_match = max(fuzzy_matches, key=lambda x: x[2])
        return best_match[0], "fuzzy"
    return "", ""

def parse_email_chain(text):
    email_chunks = split_emails(text)
    
    parsed = []
    for i, email in enumerate(email_chunks):
        sender = extract_field(email, "From")
        receiver = extract_field(email, "To")
        cc = extract_field(email, "Cc")
        bcc = extract_field(email, "Bcc")
        subject = extract_field(email, "Subject")
        date_raw = extract_field(email, "Sent")

        dt = parse_date_time(date_raw)
        date_str = dt.date().isoformat() if dt else ""
        time_str = dt.time().isoformat() if dt else ""
        
        body = extract_body(email)

        approval_statement, approval_type = find_matching_statement(body, APPROVER_KEYWORDS.keys())
        request_statement, request_type = find_matching_statement(body, REQUESTER_KEYWORDS.keys())

        parsed.append({
            "Email Sequence": i + 1,
            "Sender": sender,
            "Receiver": receiver,
            "cc": cc,
            "bcc": bcc,
            "subject": subject,
            "email body": body,
            "approval statement": approval_statement,
            "approval match type": approval_type,
            "request statement": request_statement,
            "request match type": request_type,
            "datetime": dt,
            "date": date_str,
            "time": time_str
        })

    # Sort emails by datetime to ensure proper ordering
    parsed = sorted(parsed, key=lambda x: x["datetime"] if x["datetime"] else datetime.min)
    
    # Reassign sequence numbers based on sorted order
    for i, email in enumerate(parsed):
        email["Email Sequence"] = i + 1
    
    return parsed

def extract_all_participants(parsed_emails):
    """Compile a list of all unique participants in the email chain (names only)"""
    participants = set()
    
    for email in parsed_emails:
        # Process sender
        sender = clean_participant_name(email["Sender"])
        if sender:
            participants.add(sender)
        
        # Process receivers
        receivers = [clean_participant_name(r) for r in email["Receiver"].split(',') if email["Receiver"]]
        for receiver in receivers:
            if receiver:
                participants.add(receiver)
        
        # Process CC
        cc = [clean_participant_name(c) for c in email["cc"].split(',')] if email["cc"] else []
        for c in cc:
            if c:
                participants.add(c)
        
        # Process BCC
        bcc = [clean_participant_name(b) for b in email["bcc"].split(',')] if email["bcc"] else []
        for b in bcc:
            if b:
                participants.add(b)
    
    return sorted(participants)

def calculate_role_scores(parsed_emails):
    role_scores = {}
    total_emails = len(parsed_emails)
    
    # Get thread time bounds
    valid_emails = [e for e in parsed_emails if e["datetime"]]
    if valid_emails:
        thread_start = min(e["datetime"] for e in valid_emails)
        thread_end = max(e["datetime"] for e in valid_emails)
    else:
        thread_start = thread_end = None
    
    # Get first sender based on datetime
    first_email = min(parsed_emails, key=lambda x: x["datetime"] if x["datetime"] else datetime.max)
    first_sender = clean_participant_name(first_email["Sender"])

    for i, email in enumerate(parsed_emails):
        sender = clean_participant_name(email["Sender"])
        body = email["email body"].lower()
        subject = email["subject"].lower()
        
        if not sender:
            continue
            
        if sender not in role_scores:
            role_scores[sender] = {"approver": 0, "requester": 0, "details": []}
        
        # Enhanced Temporal Scoring (20% weight)
        # Calculate position-based weights
        pos_weight_req, pos_weight_app = hybrid_temporal_weight(i, total_emails)
        
        # Calculate time-decay weights if we have datetimes
        if email["datetime"] and thread_start and thread_end:
            time_weight_req, time_weight_app = time_decay_weight(
                email["datetime"], thread_start, thread_end
            )
        else:
            time_weight_req, time_weight_app = 0.5, 0.5
        
        # Combine weights (50% position, 50% time decay)
        combined_req = 0.5 * pos_weight_req + 0.5 * time_weight_req
        combined_app = 0.5 * pos_weight_app + 0.5 * time_weight_app
        
        # Apply to scores (20% of total score)
        temporal_requester_score = 20 * combined_req
        temporal_approver_score = 20 * combined_app
        
        role_scores[sender]["requester"] += temporal_requester_score
        role_scores[sender]["approver"] += temporal_approver_score
        role_scores[sender]["details"].append(
            f"+{temporal_requester_score:.1f} (temporal requester weight)")
        role_scores[sender]["details"].append(
            f"+{temporal_approver_score:.1f} (temporal approver weight)")
        
        # [Rest of your existing scoring logic remains exactly the same]
        # ULTRA WINNER BONUS, HARD RULE, Keyword Scoring, etc.
        
    return role_scores, first_sender

def identify_requester_approver(parsed_emails):
    if not parsed_emails:
        return "", "", {}, None
    
    scores, first_sender = calculate_role_scores(parsed_emails)
    
    # Get potential approvers (excluding first sender)
    potential_approvers = [
        (sender, score["approver"]) 
        for sender, score in scores.items() 
        if sender != first_sender and score["approver"] > 0
    ]
    potential_approvers.sort(key=lambda x: x[1], reverse=True)
    
    # Get potential requesters
    potential_requesters = [
        (sender, score["requester"]) 
        for sender, score in scores.items() 
        if score["requester"] > 0
    ]
    potential_requesters.sort(key=lambda x: x[1], reverse=True)
    
    # Determine roles
    approver = potential_approvers[0][0] if potential_approvers else ""
    requester = first_sender if any(
        sender == first_sender for sender, _ in potential_requesters
    ) else (potential_requesters[0][0] if potential_requesters else "")
    
    # Fallback if no approver found
    if not approver and potential_requesters:
        for email in reversed(parsed_emails):
            current_sender = clean_participant_name(email["Sender"])
            if current_sender != requester and current_sender != first_sender:
                approver = current_sender
                scores[approver]["details"].append(
                    "+50 (fallback: last non-requester sender)")
                scores[approver]["approver"] += 50
                break
    
    return requester, approver, scores, first_sender

def create_user_role_mapping(parsed_emails, requester, approver):
    """Create a mapping of all participants to their roles (using cleaned names)"""
    participants = extract_all_participants(parsed_emails)
    role_mapping = []
    
    for participant in participants:
        if participant == requester:
            role = "Requester"
        elif participant == approver:
            role = "Approver"
        else:
            role = "Participant"
        role_mapping.append({"Name": participant, "Role": role})
    
    return role_mapping

def main():
    st.title("Email Role Classifier")
    st.markdown("""
    This app analyzes email threads to identify the most likely requester and approver roles.
    Upload a text file containing email messages in standard format (with From:, To:, Subject:, etc. headers).
    """)
    
    uploaded_file = st.file_uploader("Upload email text file", type=["txt"])
    
    if uploaded_file is not None:
        email_text = uploaded_file.read().decode("utf-8")
        
        with st.spinner("Analyzing email thread..."):
            parsed_emails = parse_email_chain(email_text)
            
            if not parsed_emails:
                st.error("No emails could be parsed from the input file.")
                return
            
            requester, approver, scores, first_sender = identify_requester_approver(parsed_emails)
            role_mapping = create_user_role_mapping(parsed_emails, requester, approver)
            
        st.success("Analysis complete!")
        
        # Display results
        st.subheader("Identified Roles")
        col1, col2 = st.columns(2)
        col1.metric("Requester", requester)
        col2.metric("Approver", approver)
        
        st.write(f"First sender in thread: {first_sender}")
        
        # Show participants and roles
        st.subheader("All Participants")
        st.dataframe(role_mapping)
        
        # Show detailed scores
        st.subheader("Detailed Role Scores")
        for sender, score_data in scores.items():
            with st.expander(f"{sender}"):
                st.write(f"**Approver Score:** {score_data['approver']}")
                st.write(f"**Requester Score:** {score_data['requester']}")
                st.write("**Score Components:**")
                for detail in score_data["details"]:
                    st.write(f"- {detail}")
        
        # Show parsed emails
        st.subheader("Parsed Emails")
        for email in parsed_emails:
            with st.expander(f"Email #{email['Email Sequence']}: {email['subject']}"):
                cols = st.columns(2)
                cols[0].write(f"**From:** {email['Sender']}")
                cols[1].write(f"**Date:** {email['date']} {email['time']}")
                st.write(f"**To:** {email['Receiver']}")
                if email["cc"]:
                    st.write(f"**Cc:** {email['cc']}")
                if email["bcc"]:
                    st.write(f"**Bcc:** {email['bcc']}")
                
                st.write("**Body:**")
                st.text(email["email body"])
                
                if email["approval statement"]:
                    st.write(f"**Approval Statement:** {email['approval statement']} ({email['approval match type']} match)")
                if email["request statement"]:
                    st.write(f"**Request Statement:** {email['request statement']} ({email['request match type']} match)")
        
        # Download buttons
        st.subheader("Download Results")
        
        # Save role mapping to CSV
        csv_data = "\n".join([f"{row['Name']},{row['Role']}" for row in role_mapping])
        st.download_button(
            label="Download User Roles (CSV)",
            data=f"Name,Role\n{csv_data}",
            file_name="Email_Users.csv",
            mime="text/csv"
        )
        
        # Save summary to text file
        summary_text = "=== ROLE ANALYSIS SUMMARY ===\n"
        summary_text += f"First Sender (can't be approver): {first_sender}\n"
        summary_text += f"Identified Requester: {requester}\n"
        summary_text += f"Identified Approver: {approver}\n\n"
        
        summary_text += "=== DETAILED SCORE BREAKDOWN ===\n"
        for sender, score_data in scores.items():
            summary_text += f"\nSender: {sender}\n"
            summary_text += f"Total Approver Score: {score_data['approver']}\n"
            summary_text += f"Total Requester Score: {score_data['requester']}\n"
            summary_text += "Score Components:\n"
            for detail in score_data["details"]:
                summary_text += f"  - {detail}\n"
        
        st.download_button(
            label="Download Summary (TXT)",
            data=summary_text,
            file_name="email_role_summary.txt",
            mime="text/plain"
        )

if __name__ == "__main__":
    main()
