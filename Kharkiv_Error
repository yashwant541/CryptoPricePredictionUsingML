import numpy as np
np.int = int
np.float = float

import math
import nltk
import re
import csv
import sys
import os
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import string
from dateutil.parser import parse as parse_date
# Setup NLTK path (optional: update as needed)
nltk.data.path.append(r'C:\Users\2011747\nltk_library')
from fuzzywuzzy import fuzz



# Weighted keyword lists
APPROVER_KEYWORDS = {
    "approved": 100, "granted": 95, "confirmed": 90, "accepted": 90,
    "authorized": 95, "approve": 80, "confirm": 80, "accept": 80,
    "approval": 50, "clearance": 70, "endorsed": 75, "signed off": 85,
    "cleared": 80, "validated": 85, "ratified": 90, "sanctioned": 90
}

REQUESTER_KEYWORDS = {
    "request": 100, "require": 95, "seek approval": 90, "need approval": 90,
    "asking": 80, "petition": 75, "approval": 30, "pending": 60,
    "remind": 70, "follow up": 65, "submitted": 85, "application": 80,
    "awaiting": 75, "petition": 70, "seeking": 85
}

PRONOUN_PATTERNS = [
    (r"\byou\b.*\bapprove\b", "approver", 50),
    (r"\byour\b.*\bapproval\b", "approver", 40),
    (r"\bplease\b.*\bapprove\b", "approver", 60),
    (r"\bi\b.*\brequest\b", "requester", 55),
    (r"\bwe\b.*\brequest\b", "requester", 50),
    (r"\bplease\b.*\breview\b", "approver", 40),
    (r"\bkindly\b.*\bapprove\b", "approver", 70)
]

# Enhanced Temporal Weighting Functions
def sigmoid(x, steepness=10, midpoint=0.5):
    """Smooth S-curve transition between requester and approver weights"""
    return 1 / (1 + math.exp(-steepness * (x - midpoint)))

def logarithmic_weight(position, total_emails):
    """More sensitive to early positions in thread"""
    if total_emails <= 1:
        return 0.5
    normalized_pos = position / (total_emails - 1)
    return 1 - math.log(1 + (1 - normalized_pos) * 9, 10)

def hybrid_temporal_weight(position, total_emails, sigmoid_ratio=0.7):
    """Combine both sigmoid and logarithmic weights"""
    if total_emails <= 1:
        return (0.5, 0.5)
    
    normalized_pos = position / (total_emails - 1)
    req_sig, app_sig = (1 - sigmoid(normalized_pos)), sigmoid(normalized_pos)
    log_weight = logarithmic_weight(position, total_emails)
    
    # Blend them
    requester_weight = sigmoid_ratio * (1 - log_weight) + (1 - sigmoid_ratio) * req_sig
    approver_weight = sigmoid_ratio * log_weight + (1 - sigmoid_ratio) * app_sig
    
    # Normalize
    total = requester_weight + approver_weight
    return (requester_weight/total, approver_weight/total)

def time_decay_weight(email_datetime, thread_start, thread_end, half_life_hours=24):
    """Weight based on absolute time differences"""
    total_span = (thread_end - thread_start).total_seconds()
    if total_span <= 0:
        return (0.5, 0.5)
    
    elapsed = (email_datetime - thread_start).total_seconds()
    decay_factor = 0.5 ** (elapsed / (half_life_hours * 3600))
    return (decay_factor, 1 - decay_factor)

def split_emails(raw_text):
    parts = re.split(r"(?=^From: )", raw_text, flags=re.IGNORECASE | re.MULTILINE)
    if parts and not parts[0].strip().lower().startswith("from:"):
        first = parts.pop(0)
        parts = [first] + parts
    return parts

def extract_field(email, field):
    pattern = rf"{field}:(.*)"
    match = re.search(pattern, email, re.IGNORECASE)
    return match.group(1).strip() if match else ""

def parse_date_time(date_str):
    if not date_str:
        return None
    try:
        return datetime.strptime(date_str.strip(), "%A, %B %d, %Y %I:%M %p")
    except Exception:
        return None

def extract_body(email):
    split_point = re.search(r"\n\s*\n", email)
    return email[split_point.end():].strip() if split_point else ""

def clean_participant_name(raw_name):
    """Extract clean name from potentially messy strings, removing all email addresses"""
    if not raw_name:
        return ""
    
    # Remove email addresses (anything in angle brackets)
    cleaned = re.sub(r'<[^>]+>', '', raw_name)
    # Remove any remaining special characters except spaces and commas
    cleaned = re.sub(r'[;"\']', '', cleaned)
    # Normalize whitespace and remove leading/trailing spaces
    cleaned = ' '.join(cleaned.split()).strip()
    # Remove trailing commas
    cleaned = cleaned.rstrip(',')
    return cleaned

def find_matching_statement(email_body, keywords, threshold=80):
    sentences = re.split(r'(?<=[.!?])\s+', email_body.strip())
    exact_matches = []
    fuzzy_matches = []

    for sentence in sentences:
        clean_sentence = sentence.lower()
        for kw in keywords:
            if kw in clean_sentence:
                exact_matches.append(sentence.strip())
            else:
                ratio = fuzz.partial_ratio(kw, clean_sentence)
                if ratio >= threshold:
                    fuzzy_matches.append((sentence.strip(), kw, ratio))

    if exact_matches:
        return "; ".join(exact_matches), "exact"
    elif fuzzy_matches:
        best_match = max(fuzzy_matches, key=lambda x: x[2])
        return best_match[0], "fuzzy"
    return "", ""

def parse_email_chain(text):
    email_chunks = split_emails(text)
    
    parsed = []
    for i, email in enumerate(email_chunks):
        sender = extract_field(email, "From")
        receiver = extract_field(email, "To")
        cc = extract_field(email, "Cc")
        bcc = extract_field(email, "Bcc")
        subject = extract_field(email, "Subject")
        date_raw = extract_field(email, "Sent")

        dt = parse_date_time(date_raw)
        date_str = dt.date().isoformat() if dt else ""
        time_str = dt.time().isoformat() if dt else ""
        
        body = extract_body(email)

        approval_statement, approval_type = find_matching_statement(body, APPROVER_KEYWORDS.keys())
        request_statement, request_type = find_matching_statement(body, REQUESTER_KEYWORDS.keys())

        parsed.append({
            "Email Sequence": i + 1,
            "Sender": sender,
            "Receiver": receiver,
            "cc": cc,
            "bcc": bcc,
            "subject": subject,
            "email body": body,
            "approval statement": approval_statement,
            "approval match type": approval_type,
            "request statement": request_statement,
            "request match type": request_type,
            "datetime": dt,
            "date": date_str,
            "time": time_str
        })

    # Sort emails by datetime to ensure proper ordering
    parsed = sorted(parsed, key=lambda x: x["datetime"] if x["datetime"] else datetime.min)
    
    # Reassign sequence numbers based on sorted order
    for i, email in enumerate(parsed):
        email["Email Sequence"] = i + 1
    
    return parsed

def extract_all_participants(parsed_emails):
    """Compile a list of all unique participants in the email chain (names only)"""
    participants = set()
    
    for email in parsed_emails:
        # Process sender
        sender = clean_participant_name(email["Sender"])
        if sender:
            participants.add(sender)
        
        # Process receivers
        receivers = [clean_participant_name(r) for r in email["Receiver"].split(',') if email["Receiver"]]
        for receiver in receivers:
            if receiver:
                participants.add(receiver)
        
        # Process CC
        cc = [clean_participant_name(c) for c in email["cc"].split(',')] if email["cc"] else []
        for c in cc:
            if c:
                participants.add(c)
        
        # Process BCC
        bcc = [clean_participant_name(b) for b in email["bcc"].split(',')] if email["bcc"] else []
        for b in bcc:
            if b:
                participants.add(b)
    
    return sorted(participants)

def calculate_role_scores(parsed_emails):
    role_scores = {}
    total_emails = len(parsed_emails)
    
    # Get thread time bounds
    valid_emails = [e for e in parsed_emails if e["datetime"]]
    if valid_emails:
        thread_start = min(e["datetime"] for e in valid_emails)
        thread_end = max(e["datetime"] for e in valid_emails)
    else:
        thread_start = thread_end = None
    
    # Get first sender based on datetime
    first_email = min(parsed_emails, key=lambda x: x["datetime"] if x["datetime"] else datetime.max)
    first_sender = clean_participant_name(first_email["Sender"])

    for i, email in enumerate(parsed_emails):
        sender = clean_participant_name(email["Sender"])
        body = email["email body"].lower()
        subject = email["subject"].lower()
        
        if not sender:
            continue
            
        if sender not in role_scores:
            role_scores[sender] = {"approver": 0, "requester": 0, "details": []}
        
        # Enhanced Temporal Scoring (20% weight)
        # Calculate position-based weights
        pos_weight_req, pos_weight_app = hybrid_temporal_weight(i, total_emails)
        
        # Calculate time-decay weights if we have datetimes
        if email["datetime"] and thread_start and thread_end:
            time_weight_req, time_weight_app = time_decay_weight(
                email["datetime"], thread_start, thread_end
            )
        else:
            time_weight_req, time_weight_app = 0.5, 0.5
        
        # Combine weights (50% position, 50% time decay)
        combined_req = 0.5 * pos_weight_req + 0.5 * time_weight_req
        combined_app = 0.5 * pos_weight_app + 0.5 * time_weight_app
        
        # Apply to scores (20% of total score)
        temporal_requester_score = 20 * combined_req
        temporal_approver_score = 20 * combined_app
        
        role_scores[sender]["requester"] += temporal_requester_score
        role_scores[sender]["approver"] += temporal_approver_score
        role_scores[sender]["details"].append(
            f"+{temporal_requester_score:.1f} (temporal requester weight)")
        role_scores[sender]["details"].append(
            f"+{temporal_approver_score:.1f} (temporal approver weight)")
        
        # [Rest of your existing scoring logic remains exactly the same]
        # ULTRA WINNER BONUS, HARD RULE, Keyword Scoring, etc.
        
    return role_scores, first_sender
def identify_requester_approver(parsed_emails):
    if not parsed_emails:
        return "", "", {}, None
    
    scores, first_sender = calculate_role_scores(parsed_emails)
    
    # Get potential approvers (excluding first sender)
    potential_approvers = [
        (sender, score["approver"]) 
        for sender, score in scores.items() 
        if sender != first_sender and score["approver"] > 0
    ]
    potential_approvers.sort(key=lambda x: x[1], reverse=True)
    
    # Get potential requesters
    potential_requesters = [
        (sender, score["requester"]) 
        for sender, score in scores.items() 
        if score["requester"] > 0
    ]
    potential_requesters.sort(key=lambda x: x[1], reverse=True)
    
    # Determine roles
    approver = potential_approvers[0][0] if potential_approvers else ""
    requester = first_sender if any(
        sender == first_sender for sender, _ in potential_requesters
    ) else (potential_requesters[0][0] if potential_requesters else "")
    
    # Fallback if no approver found
    if not approver and potential_requesters:
        for email in reversed(parsed_emails):
            current_sender = clean_participant_name(email["Sender"])
            if current_sender != requester and current_sender != first_sender:
                approver = current_sender
                scores[approver]["details"].append(
                    "+50 (fallback: last non-requester sender)")
                scores[approver]["approver"] += 50
                break
    
    return requester, approver, scores, first_sender

def save_to_csv(parsed_emails, output_file="parsed_emails.csv"):
    if not parsed_emails:
        print("‚ùå No emails found.")
        return

    fieldnames = parsed_emails[0].keys()
    with open(output_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in parsed_emails:
            writer.writerow(row)
    print(f"‚úÖ CSV saved: {output_file}")

def save_summary(parsed_emails, scores, summary_file="summary_output.txt"):
    requester, approver, _, first_sender = identify_requester_approver(parsed_emails)
    
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write("=== ROLE ANALYSIS SUMMARY ===\n")
        f.write(f"First Sender (can't be approver): {first_sender}\n")
        f.write(f"Identified Requester: {requester}\n")
        f.write(f"Identified Approver: {approver}\n\n")
        
        f.write("=== DETAILED SCORE BREAKDOWN ===\n")
        for sender, score_data in scores.items():
            f.write(f"\nSender: {sender}\n")
            f.write(f"Total Approver Score: {score_data['approver']}\n")
            f.write(f"Total Requester Score: {score_data['requester']}\n")
            f.write("Score Components:\n")
            for detail in score_data["details"]:
                f.write(f"  - {detail}\n")
        
        f.write("\n=== EMAIL CHAIN DETAILS ===\n")
        for email in parsed_emails:
            f.write(f"\n--- Email #{email['Email Sequence']} ---\n")
            f.write(f"From: {email['Sender']}\n")
            f.write(f"To: {email['Receiver']}\n")
            f.write(f"Subject: {email['subject']}\n")
            f.write(f"Date: {email['date']} {email['time']}\n")
            f.write(f"Approval Statement: {email['approval statement']}\n")
            f.write(f"Request Statement: {email['request statement']}\n")
    
    print(f"‚úÖ Summary saved: {summary_file}")

def create_user_role_mapping(parsed_emails, requester, approver):
    """Create a mapping of all participants to their roles (using cleaned names)"""
    participants = extract_all_participants(parsed_emails)
    role_mapping = []
    
    for participant in participants:
        if participant == requester:
            role = "Requester"
        elif participant == approver:
            role = "Approver"
        else:
            role = "Participant"
        role_mapping.append({"Name": participant, "Role": role})
    
    return role_mapping

def save_user_roles_to_csv(role_mapping, filename="Email_Users.csv"):
    """Save the user role mapping to a CSV file"""
    with open(filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["Name", "Role"])
        writer.writeheader()
        writer.writerows(role_mapping)
    print(f"‚úÖ User roles saved: {filename}")

def main(input_file):
    if not os.path.exists(input_file):
        print(f"‚ùå File not found: {input_file}")
        return
    
    with open(input_file, "r", encoding="utf-8") as f:
        email_text = f.read()
    
    parsed_emails = parse_email_chain(email_text)
    if not parsed_emails:
        print("‚ùå No emails could be parsed from the input file.")
        return
    
    save_to_csv(parsed_emails)
    
    requester, approver, scores, first_sender = identify_requester_approver(parsed_emails)
    save_summary(parsed_emails, scores)
    
    # Create and save user roles
    role_mapping = create_user_role_mapping(parsed_emails, requester, approver)
    save_user_roles_to_csv(role_mapping)
    
    print("\nüîç Identified Roles:")
    print(f"First Sender (can't be approver): {first_sender}")
    print(f"Requester: {requester}")
    print(f"Approver: {approver}")
    print(f"\nUser roles saved to Email_Users.csv")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python email_parse.py <input_email_text_file>")
    else:
        main(sys.argv[1])
