def chat_file_stats_log(df_clean: pd.DataFrame, df_excluded: pd.DataFrame, chat_file: str) -> Tuple[dict, pd.Series,
                                                                                                    pd.Series, pd.DataFrame]:
    """
    For a given chat log, generate summary information:
    - log: key file information, Python libraries versions and number of rows at different stages
    - keywords in chat log summary
    - lines excluded from chat log (from data cleaning) summary
    - same keywords summary

    Args:
        df_clean (): chat log after data cleaning.
        df_excluded (): lines excluded from chat log due to data cleaning.
        chat_file (): name of the chat file.

    Returns:
        log, keywords_match_summary, exclusions_summary, same_keywords_summary
    """
    # create a log with key information about Python libraries and number of rows at different stages
    log = {'file_name': chat_file,
           'Script_run_date': str(date.today()),
           'Python_version_&_Libraries': system_info(("Pandas",)),
           'Total_rows_in_raw_file': len(df_clean) + len(df_excluded),
           'Total_rows_excluded_due_to_cleaning': len(df_excluded),
           'Total_rows_after_data_cleaning': len(df_clean),
           'Nb_rows_with_a_match_from_lexicon': df_clean['GIA_keywords_match_flag'].sum(),
           'Nb_cases_to_investigate': df_clean['GIA_group_case'].nunique(dropna=True)}

    # keywords match summary
    keywords_match_summary = df_clean['GIA_keywords_match'].value_counts()

    # same keywords summary
    same_keywords_summary = summarize_same_keywords(df_clean['GIA_keywords_match'])

    # exclusions summary
    exclusions_summary = df_excluded.loc[:, [col for col in df_excluded.columns if "excl" in col]].sum(
        axis=0).sort_values(ascending=False)

    return log, keywords_match_summary, exclusions_summary, same_keywords_summary


def summarize_same_keywords(keywords_series: pd.Series) -> pd.DataFrame:
    """
    Summarize same keywords in the given series.

    Args:
        keywords_series: Series containing keywords.

    Returns:
        DataFrame with the summary of same keywords.
    """
    # Convert the keywords to lowercase and remove duplicates
    lowercase_keywords = keywords_series.str.lower().dropna().unique()

    # Create a DataFrame to store the summary
    same_keywords_summary = pd.DataFrame(columns=['Keywords_match', 'Same_keywords'])

    # Iterate through each unique lowercase keyword
    for keyword in lowercase_keywords:
        # Find the variations of the keyword in the original series
        variations = keywords_series[keywords_series.str.lower() == keyword].unique()
        same_keywords_summary = same_keywords_summary.append(
            {'Keywords_match': keyword, 'Same_keywords': variations}, ignore_index=True)

    return same_keywords_summary


def save_final_processed_files(user_parameters: dict, df_clean: pd.DataFrame, df_excluded: pd.DataFrame, file_name: str,
                               file_number: int, log: dict, keywords_match_summary: pd.Series,
                               exclusions_summary: pd.Series, same_keywords_summary: pd.DataFrame) -> None:
    """
    Save the different output files (lines to investigate, clean chat log, excluded lines, log and summary table).
    Args:
        user_parameters (): user parameters.
        df_clean (): chat log after data cleaning.
        df_excluded (): excluded chat log lines (following data cleaning).
        file_name (): name of the chat log.
        file_number (): file number in the file processing.
        log (): script log.
        keywords_match_summary (): summary table on keywords in lexicon found in the chat log.
        exclusions_summary (): summary table about excluded lines.
        same_keywords_summary: summary table for same keywords.

    Returns:
        None
    """
    # location where to save output files
    file_path = os.path.join(user_parameters['output_path'], str(file_number))

    # for each chat file, create a new folder (file names are too long to be used as folder names)
    try:
        os.mkdir(file_path)
    except FileExistsError:  # if folder already exists
        pass

    # save excluded lines
    df_excluded.to_excel(f"{file_path}//{file_name}-EXCLUDED.xlsx")

    # save clean filtered df
    df_clean.to_excel(f"{file_path}//{file_name}-CLEAN.xlsx")

    # save exceptions to investigate (flagged by case number, ignore all the cleaning columns for clarity)
    m = ~df_clean['GIA_group_case'].isnull()
    df_clean.loc[m, [col for col in df_clean.columns if "excl_" not in col]].to_excel(
        f"{file_path}//{file_name}-TO_INVESTIGATE.xlsx")

    # save log, keywords_match_summary, exclusions_summary, and same_keywords_summary into a single Excel file
    with pd.ExcelWriter(f"{file_path}//{file_name}-SUMMARY.xlsx") as writer:
        pd.DataFrame.from_dict(log, orient='index', columns=['File_Name']).reset_index().to_excel(writer,
                                                                                                  sheet_name='Log',
                                                                                                  index=False)
        keywords_match_summary.to_excel(writer, header=['Keywords_match'], sheet_name='Keywords_summary')
        exclusions_summary.to_excel(writer, header=['Exclusion_rules'], sheet_name='Exclusions_summary')
        same_keywords_summary.to_excel(writer, header=['Same_keywords'], sheet_name='Same_Keywords_summary')
