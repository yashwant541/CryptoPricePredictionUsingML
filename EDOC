import pandas as pd
import re
from collections import defaultdict

# Built-in stopwords
STOPWORDS = {
    'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'when',
    'what', 'where', 'from', 'how', 'of', 'for', 'to', 'at', 'in', 'on',
    'that', 'this', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',
    'can', 'could', 'may', 'might', 'must', 'shall', 'which', 'who', 'whom',
    'whose', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her',
    'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their', 'mine', 'yours',
    'hers', 'ours', 'theirs', 'myself', 'yourself', 'himself', 'herself',
    'itself', 'ourselves', 'yourselves', 'themselves'
}

def select_column(df, file_type):
    """Let user select a column from dataframe"""
    print(f"\nColumns in {file_type} file:")
    for i, col in enumerate(df.columns, 1):
        print(f"{i}. {col}")
    
    while True:
        try:
            col_num = int(input(f"Select {file_type} column number: ")) - 1
            if 0 <= col_num < len(df.columns):
                return df.columns[col_num]
            print("Invalid column number. Try again.")
        except ValueError:
            print("Please enter a valid number.")

def tokenize(text):
    """Advanced tokenizer that handles contractions and hyphenated words"""
    return re.findall(r"\b[\w'-]+\b", str(text).lower())

def analyze_text(text, keyword_map):
    """Analyze text with keyword mapping"""
    words = tokenize(text)
    matches = []
    found_stopwords = set()
    clean_words = []
    
    for word in words:
        if word in STOPWORDS:
            found_stopwords.add(word)
            continue
        
        clean_words.append(word)
        if word in keyword_map:
            matches.extend(keyword_map[word])
    
    # Check for partial matches
    clean_text = ' '.join(clean_words)
    for keyword, mapped_values in keyword_map.items():
        if len(keyword) > 2 and keyword in clean_text:
            if not any(kw == keyword for kw, _ in matches):
                matches.extend((f"{keyword}*", mv) for mv in mapped_values)
    
    return {
        'matches': matches,
        'stopwords': sorted(found_stopwords),
        'word_count': len(words),
        'clean_word_count': len(clean_words)
    }

def main():
    print("üîç Keyword Mapper Analyzer")
    print("==========================")
    
    # File input
    data_file = input("Enter main data file path: ")
    keywords_file = input("Enter keywords file path: ")
    
    try:
        # Load data
        df_data = pd.read_excel(data_file)
        df_keywords = pd.read_excel(keywords_file)
        
        # Column selection
        print("\nFor MAIN DATA FILE:")
        data_col = select_column(df_data, "text data")
        
        print("\nFor KEYWORDS FILE:")
        keyword_col = select_column(df_keywords, "keywords")
        mapped_col = select_column(df_keywords, "mapped values")
        
        # Create keyword mapping (keyword -> list of mapped values)
        keyword_map = defaultdict(list)
        for _, row in df_keywords.dropna(subset=[keyword_col]).iterrows():
            keyword = str(row[keyword_col]).lower().strip()
            if keyword and keyword not in STOPWORDS:
                keyword_map[keyword].append(row[mapped_col])
        
        # Process data
        results = []
        stopword_stats = defaultdict(int)
        
        for _, row in df_data.iterrows():
            text = row[data_col]
            analysis = analyze_text(text, keyword_map)
            
            # Update statistics
            for sw in analysis['stopwords']:
                stopword_stats[sw] += 1
            
            # Get unique mapped values from matches
            matched_values = list(set(mv for _, mv in analysis['matches']))
            
            results.append({
                'Original_Text': text,
                'Has_Match': bool(analysis['matches']),
                'Matched_Keywords': ', '.join(kw for kw, _ in analysis['matches']),
                'Mapped_Values': ', '.join(str(v) for v in matched_values) if matched_values else None,
                'Stopwords_Found': ', '.join(analysis['stopwords']) if analysis['stopwords'] else None,
                'Match_Count': len(analysis['matches']),
                'Stopword_Count': len(analysis['stopwords']),
                'Word_Count': analysis['word_count'],
                'Clean_Word_Count': analysis['clean_word_count']
            })
        
        # Create output DataFrames
        df_output = pd.concat([
            df_data,
            pd.DataFrame(results)[['Has_Match', 'Matched_Keywords', 'Mapped_Values',
                                 'Stopwords_Found', 'Match_Count', 'Stopword_Count',
                                 'Word_Count', 'Clean_Word_Count']]
        ], axis=1)
        
        df_stopwords = pd.DataFrame(
            sorted(stopword_stats.items(), key=lambda x: (-x[1], x[0])),
            columns=['Stopword', 'Frequency']
        )
        
        # Save outputs
        output_name = input("\nEnter output file base name (without extension): ")
        df_output.to_excel(f"{output_name}_results.xlsx", index=False)
        df_stopwords.to_excel(f"{output_name}_stopwords.xlsx", index=False)
        
        # Summary
        total_matches = df_output['Has_Match'].sum()
        unique_mapped = df_output['Mapped_Values'].nunique()
        
        print("\nüìä Analysis Complete")
        print("===================")
        print(f"Processed {len(df_output)} records")
        print(f"Matching records: {total_matches} ({total_matches/len(df_output)*100:.1f}%)")
        print(f"Unique mapped values found: {unique_mapped}")
        print(f"\nOutput files created:")
        print(f"- {output_name}_results.xlsx (Main results with mapped values)")
        print(f"- {output_name}_stopwords.xlsx (Stopword statistics)")
        
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")

if __name__ == "__main__":
    main()
