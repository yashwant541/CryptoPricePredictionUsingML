import dataiku
import extractmsg
import pandas as pd
import tempfile
import shutil
import os
import sys
import re
from datetime import datetime
from email.utils import parsedate_to_datetime

# -----------------------------
# CONFIGURATION
# -----------------------------
INPUT_FOLDER_ID = "YOUR_INPUT_FOLDER_ID"
OUTPUT_FOLDER_ID = "YOUR_OUTPUT_FOLDER_ID"

input_folder = dataiku.Folder(INPUT_FOLDER_ID)
output_folder = dataiku.Folder(OUTPUT_FOLDER_ID)

# -----------------------------
# LOGGING
# -----------------------------
def log(message):
    print(f"[LOG] {message}", file=sys.stderr, flush=True)

# -----------------------------
# CONVERT MSG TO TXT
# -----------------------------
def msg_to_txt(msg_file_path, txt_file_path):
    """
    Convert .msg file to .txt format with structured content
    """
    try:
        msg = extractmsg.Message(msg_file_path)
        
        with open(txt_file_path, 'w', encoding='utf-8') as txt_file:
            # Write headers in a structured format
            txt_file.write("=== EMAIL HEADERS ===\n")
            txt_file.write(f"From: {msg.sender or ''}\n")
            txt_file.write(f"To: {msg.to or ''}\n")
            txt_file.write(f"CC: {msg.cc or ''}\n")
            txt_file.write(f"BCC: {msg.bcc or ''}\n")
            txt_file.write(f"Subject: {msg.subject or ''}\n")
            txt_file.write(f"Date: {msg.date or ''}\n")
            txt_file.write(f"Message-ID: {getattr(msg, 'message_id', '')}\n")
            
            # Write body
            txt_file.write("\n=== EMAIL BODY ===\n")
            txt_file.write(msg.body or '')
            
            # Write attachments list
            txt_file.write("\n=== ATTACHMENTS ===\n")
            attachments = []
            for att in msg.attachments:
                att_name = att.longFilename or "Unknown"
                attachments.append(att_name)
                txt_file.write(f"{att_name}\n")
                
            msg.close()
            
        return len(attachments)
        
    except Exception as e:
        log(f"âŒ Error converting .msg to .txt: {str(e)}")
        # Create error file
        with open(txt_file_path, 'w', encoding='utf-8') as txt_file:
            txt_file.write(f"ERROR: {str(e)}\n")
        return 0

# -----------------------------
# PARSE TXT TO STRUCTURED DATA
# -----------------------------
def parse_txt_to_data(txt_file_path, original_filename):
    """
    Parse the generated .txt file back to structured data
    """
    try:
        with open(txt_file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        
        # Initialize data dictionary
        email_data = {
            "Sender": "",
            "To": "",
            "CC": "",
            "BCC": "",
            "Subject": "",
            "Date": "",
            "Body": "",
            "Attachments": "",
            "Filename": original_filename,
            "Processing_Date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Message_ID": ""
        }
        
        # Parse headers section
        headers_section = re.search(r'=== EMAIL HEADERS ===(.*?)=== EMAIL BODY ===', content, re.DOTALL)
        if headers_section:
            headers_text = headers_section.group(1)
            
            # Extract individual headers
            sender_match = re.search(r'From:\s*(.*)', headers_text)
            to_match = re.search(r'To:\s*(.*)', headers_text)
            cc_match = re.search(r'CC:\s*(.*)', headers_text)
            bcc_match = re.search(r'BCC:\s*(.*)', headers_text)
            subject_match = re.search(r'Subject:\s*(.*)', headers_text)
            date_match = re.search(r'Date:\s*(.*)', headers_text)
            msg_id_match = re.search(r'Message-ID:\s*(.*)', headers_text)
            
            email_data["Sender"] = sender_match.group(1).strip() if sender_match else ""
            email_data["To"] = to_match.group(1).strip() if to_match else ""
            email_data["CC"] = cc_match.group(1).strip() if cc_match else ""
            email_data["BCC"] = bcc_match.group(1).strip() if bcc_match else ""
            email_data["Subject"] = subject_match.group(1).strip() if subject_match else ""
            email_data["Date"] = date_match.group(1).strip() if date_match else ""
            email_data["Message_ID"] = msg_id_match.group(1).strip() if msg_id_match else ""
        
        # Parse body section
        body_section = re.search(r'=== EMAIL BODY ===(.*?)=== ATTACHMENTS ===', content, re.DOTALL)
        if body_section:
            email_data["Body"] = body_section.group(1).strip()
        else:
            # If attachments section is missing, try to get everything after body marker
            body_section = re.search(r'=== EMAIL BODY ===(.*)', content, re.DOTALL)
            if body_section:
                email_data["Body"] = body_section.group(1).strip()
        
        # Parse attachments section
        attachments_section = re.search(r'=== ATTACHMENTS ===(.*)', content, re.DOTALL)
        if attachments_section:
            attachments_text = attachments_section.group(1).strip()
            # Clean up attachments list - remove empty lines and join with comma
            attachments_list = [att.strip() for att in attachments_text.split('\n') if att.strip()]
            email_data["Attachments"] = ", ".join(attachments_list)
        
        return [email_data]
        
    except Exception as e:
        log(f"âŒ Error parsing .txt file: {str(e)}")
        return [{
            "Sender": "",
            "To": "",
            "CC": "",
            "BCC": "",
            "Subject": "ERROR_PARSING_TXT",
            "Date": "",
            "Body": f"Error parsing .txt: {str(e)}",
            "Attachments": "",
            "Filename": original_filename,
            "Processing_Date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Error": str(e)
        }]

# -----------------------------
# PROCESS NESTED MSG FILES
# -----------------------------
def process_nested_msg_attachments(msg_file_path, original_filename):
    """
    Handle nested .msg attachments recursively and return all emails
    """
    all_emails = []
    
    try:
        msg = extractmsg.Message(msg_file_path)
        
        # Process the main email first by converting to txt and parsing
        with tempfile.NamedTemporaryFile(suffix=".txt", delete=False, mode='w', encoding='utf-8') as tmp_txt:
            tmp_txt_path = tmp_txt.name
        
        # Convert main message to txt
        msg_to_txt(msg_file_path, tmp_txt_path)
        main_emails = parse_txt_to_data(tmp_txt_path, original_filename)
        all_emails.extend(main_emails)
        
        # Process nested .msg attachments
        for att in msg.attachments:
            if att.longFilename and att.longFilename.lower().endswith(".msg"):
                log(f"  ğŸ“ Processing nested .msg: {att.longFilename}")
                with tempfile.NamedTemporaryFile(suffix=".msg", delete=False) as tmp_att_file:
                    att.save(tmp_att_file.name)
                    nested_emails = process_nested_msg_attachments(tmp_att_file.name, att.longFilename)
                    all_emails.extend(nested_emails)
                    os.unlink(tmp_att_file.name)
        
        msg.close()
        os.unlink(tmp_txt_path)
        
    except Exception as e:
        log(f"âŒ Error processing nested attachments: {str(e)}")
        all_emails.append({
            "Sender": "",
            "To": "",
            "CC": "",
            "BCC": "",
            "Subject": "ERROR_NESTED_PROCESSING",
            "Date": "",
            "Body": f"Error processing nested: {str(e)}",
            "Attachments": "",
            "Filename": original_filename,
            "Processing_Date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Error": str(e)
        })
    
    return all_emails

# -----------------------------
# MAIN PROCESS
# -----------------------------
def main():
    try:
        log("ğŸ” Searching for .msg files in input folder...")
        all_files = input_folder.list_paths_in_partition()
        msg_files = [f for f in all_files if f.lower().endswith(".msg")]

        if not msg_files:
            log("âŒ No .msg files found.")
            return

        log(f"ğŸ“§ Found {len(msg_files)} .msg file(s) to process")
        
        processed_count = 0
        error_count = 0
        
        for msg_path in msg_files:
            file_name = os.path.basename(msg_path)
            log(f"ğŸ“¨ Processing: {file_name}")
            
            try:
                # Step 1: Download .msg to temp file
                with tempfile.NamedTemporaryFile(suffix=".msg", delete=False) as tmp_msg:
                    tmp_msg_path = tmp_msg.name
                    with input_folder.get_download_stream(msg_path) as stream:
                        shutil.copyfileobj(stream, tmp_msg)
                
                # Step 2: Process main email and any nested .msg attachments
                all_emails = process_nested_msg_attachments(tmp_msg_path, file_name)
                
                # Step 3: Convert to DataFrame
                df = pd.DataFrame(all_emails)
                
                # Step 4: Save as CSV
                output_csv_name = file_name.replace(".msg", ".csv")
                
                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8-sig', newline='') as tmp_csv:
                    df.to_csv(tmp_csv, index=False)
                    tmp_csv_path = tmp_csv.name
                
                with open(tmp_csv_path, 'rb') as f:
                    output_folder.upload_stream(output_csv_name, f)
                
                # Optional: Also save the intermediate .txt file for debugging
                # txt_output_name = file_name.replace(".msg", ".txt")
                # with open(tmp_txt_path, 'rb') as f:
                #     output_folder.upload_stream(txt_output_name, f)
                
                # Clean up temp files
                os.unlink(tmp_msg_path)
                os.unlink(tmp_csv_path)
                
                processed_count += 1
                log(f"âœ… Completed: {file_name} -> {output_csv_name} ({len(all_emails)} email(s))")
                
            except Exception as e:
                error_count += 1
                log(f"âŒ Failed to process {file_name}: {str(e)}")
                
        log(f"ğŸ“Š Processing complete: {processed_count} successful, {error_count} failed")

    except Exception as e:
        log(f"ğŸ”¥ Fatal error: {str(e)}")
        raise

# -----------------------------
# EXECUTE
# -----------------------------
if __name__ == "__main__":
    main()
