def chat_file_stats_log(df_clean: pd.DataFrame, df_excluded: pd.DataFrame, chat_file: str, filename: str) -> Tuple[dict, pd.Series, pd.Series]:
    """
    For a given chat log, generate summary information:
    - log: key file information, Python libraries versions and number of rows at different stages
    - keywords in chat log summary
    - lines excluded from chat log (from data cleaning) summary

    Args:
        df_clean (): chat log after data cleaning.
        df_excluded (): lines excluded from chat log due to data cleaning.
        chat_file (): name of the chat file.
        filename (): name of the processed file.

    Returns:
        log, keywords_match_summary, exclusions_summary
    """
    # create a log with key information about Python libraries and number of rows at different stages
    log = {'file_name': chat_file,
           'Processed_File_Name': filename,  # Add filename here
           'Script_run_date': str(date.today()),
           'Python_version_&_Libraries': system_info(("Pandas",)),
           'Total_rows_in_raw_file': len(df_clean) + len(df_excluded),
           'Total_rows_excluded_due_to_cleaning': len(df_excluded),
           'Total_rows_after_data_cleaning': len(df_clean),
           'Nb_rows_with_a_match_from_lexicon': df_clean['GIA_keywords_match_flag'].sum(),
           'Nb_cases_to_investigate': df_clean['GIA_group_case'].nunique(dropna=True)}

    # keywords match summary
    # Filter out NaN values and flatten the list of keywords
    keywords_match_summary = df_clean['GIA_keywords_match'].dropna().explode().value_counts()

    # exclusions summary
    exclusions_summary = df_excluded.loc[:, [col for col in df_excluded.columns if "excl" in col]].sum(
        axis=0).sort_values(ascending=False)

    return log, keywords_match_summary, exclusions_summary


def save_final_processed_files(user_parameters: dict, df_clean: pd.DataFrame, df_excluded: pd.DataFrame, file_name: str,
                               file_number: int, log: dict, keywords_match_summary: pd.Series,
                               exclusions_summary: pd.Series, filename: str) -> None:
    """
    Save the different output files (lines to investigate, clean chat log, excluded lines, log and summary table).
    Args:
        user_parameters (): user parameters.
        df_clean (): chat log after data cleaning.
        df_excluded (): excluded chat log lines (following data cleaning).
        file_name (): name of the chat log.
        file_number (): file number in the file processing.
        log (): script log.
        keywords_match_summary (): summary table on keywords in lexicon found in the chat log.
        exclusions_summary (): summary table about excluded lines.
        filename (): name of the processed file.

    Returns:
        None
    """
    # location where to save output files
    file_path = os.path.join(user_parameters['output_path'], str(file_number))

    # for each chat file, create a new folder (file names are too long to be used as folder names)
    try:
        os.mkdir(file_path)
    except FileExistsError:  # if folder already exists
        pass

    # save excluded lines
    df_excluded.to_excel(f"{file_path}//{filename}-EXCLUDED.xlsx")

    # save clean filtered df
    df_clean.to_excel(f"{file_path}//{filename}-CLEAN.xlsx")

    # save exceptions to investigate (flagged by case number, ignore all the cleaning columns for clarity)
    m = ~df_clean['GIA_group_case'].isnull()
    df_clean.loc[m, [col for col in df_clean.columns if "excl_" not in col]].to_excel(
        f"{file_path}//{filename}-TO_INVESTIGATE.xlsx")

    # save log, keywords_match_summary, and exclusions_summary into a single Excel file
    with pd.ExcelWriter(f"{file_path}//{filename}-SUMMARY.xlsx") as writer:
        pd.DataFrame.from_dict(log, orient='index', columns=['File_Name']).reset_index().to_excel(writer,
                                                                                                  sheet_name='Log',
                                                                                                  index=False)
        keywords_match_summary.to_excel(writer, header=['Keywords_match'], sheet_name='Keywords_summary')
        exclusions_summary.to_excel(writer, header=['Exclusion_rules'], sheet_name='Exclusions_summary')


@function_timer
def run_text_analytics_chats(path: str = None, file_name: str = None) -> None:
    """
    Function to run the entire DA test for keywords analytics.

     Args:
        file_name : name of the user input file
        path : location of the user_parameters file.

    Returns:
        None
    """

    # read input data
    user_parameters, keywords_lexicon, cleaning_rules = input_data(path, file_name)

    # create the list of chat log files to process
    files_list = os.listdir(user_parameters['chat_folder_path'])

    # initialise file log number and log used to save file names with number
    file_number = 1
    file_log: dict = {}  # key - file number -> value - file name

    # process each chat file
    for chat_file_name in files_list:
        # warning to inform user about the processing progress
        print(f"file {file_number} (out of {len(files_list)}) processing has started (file name: {chat_file_name})")

        file_log[file_number] = chat_file_name

        if user_parameters['file_type'] == 'B':
            df = read_bbg_chat_file(path=user_parameters['chat_folder_path'], file_name=chat_file_name)
        elif user_parameters['file_type'] == 'S':
            df = read_skype_chat_file(path=user_parameters['chat_folder_path'], file_name=chat_file_name,
                                      skype_column=user_parameters['skype_column_name'])
        else:
            sys.exit('The file_type parameter is not properly set')

        df_clean, df_excluded = flag_lines_to_exclude(df=df, exclusions_table=cleaning_rules)
        df_clean = lexicon_matching(df=df_clean, lexicon=keywords_lexicon, user_parameters=user_parameters)
        log, keywords_match_summary, exclusion_summary = chat_file_stats_log(df_clean=df_clean, df_excluded=df_excluded,
                                                                             chat_file=chat_file_name, filename=chat_file_name)  # Pass filename here
        save_final_processed_files(user_parameters=user_parameters, df_clean=df_clean, df_excluded=df_excluded,
                                   file_name=chat_file_name, file_number=file_number, log=log,
                                   keywords_match_summary=keywords_match_summary, exclusions_summary=exclusion_summary,
                                   filename=chat_file_name)  # Pass filename here

       
