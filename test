def chat_file_stats_log(df_clean: pd.DataFrame, df_excluded: pd.DataFrame, chat_file: str, filename: str) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    For a given chat log, generate summary information:
    - log: key file information, Python libraries versions and number of rows at different stages
    - keywords in chat log summary
    - lines excluded from chat log (from data cleaning) summary

    Args:
        df_clean (): chat log after data cleaning.
        df_excluded (): lines excluded from chat log due to data cleaning.
        chat_file (): name of the chat file.
        filename (): name of the processed file.

    Returns:
        log, keywords_match_summary, exclusions_summary
    """
    # Create a dictionary for log information
    log = {
        'file_name': chat_file,
        'Processed_File_Name': filename,
        'Script_run_date': str(date.today()),
        'Python_version_&_Libraries': system_info(("Pandas",)),
        'Total_rows_in_raw_file': len(df_clean) + len(df_excluded),
        'Total_rows_excluded_due_to_cleaning': len(df_excluded),
        'Total_rows_after_data_cleaning': len(df_clean),
        'Nb_rows_with_a_match_from_lexicon': df_clean['GIA_keywords_match_flag'].sum(),
        'Nb_cases_to_investigate': df_clean['GIA_group_case'].nunique(dropna=True)
    }

    # Convert the log dictionary into a DataFrame and transpose it
    log_df = pd.DataFrame(log, index=[0]).T.reset_index()
    log_df.columns = ['Parameter', 'Value']

    # Set the index of the DataFrame to the 'Parameter' column
    log_df.set_index('Parameter', inplace=True)

    # keywords match summary
    # Filter out NaN values and flatten the list of keywords
    keywords_match_summary = df_clean['GIA_keywords_match'].dropna().explode().value_counts()

    # exclusions summary
    exclusions_summary = df_excluded.loc[:, [col for col in df_excluded.columns if "excl" in col]].sum(
        axis=0).sort_values(ascending=False)

    return log_df, keywords_match_summary, exclusions_summary
