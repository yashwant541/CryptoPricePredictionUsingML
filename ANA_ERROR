import pandas as pd
import os
import io
import re
import dataiku

# ------------------------------------------------------
# ‚öôÔ∏è CONFIGURATION
# ------------------------------------------------------
use_dataiku = True  # üîÑ Switch between Dataiku and local

if use_dataiku:
    input_folder = dataiku.Folder("XXXXXXXXX")   # Input folder
    output_folder = dataiku.Folder("XXXXXXXXX")  # Output folder
else:
    input_folder_path = r"C:\path\to\matched_clean"
    output_folder_path = r"C:\path\to\long_output"


# ------------------------------------------------------
# üß© Helper Function: Extract date from filename
# ------------------------------------------------------
def extract_date_from_filename(filename):
    parts = filename.split("_")
    if len(parts) >= 4:
        date_part = parts[3].split(".")[0]
        if re.match(r"\d{1,2}-[A-Za-z]{3}-\d{2,4}", date_part):
            return date_part
    return ""


# ------------------------------------------------------
# üß† PROCESS FUNCTION
# ------------------------------------------------------
def process_file_from_stream(stream, file_name):
    df = pd.read_csv(stream, dtype=str)
    df.columns = df.columns.str.strip()

    # Identify Tenor column
    tenor_col = next((c for c in df.columns if c.lower().strip() == "tenor"), None)
    if tenor_col is None:
        raise ValueError(f"‚ùå 'Tenor' column not found in {file_name}")

    # Define rate & category column mapping
    mapping = {
        "ATM Ask": "ATM Ask/Bid Category",
        "ATM Bid": "ATM Ask/Bid Category",
        "25d RR Mid": "25d RR Mid Category",
        "25d BF Mid": "25d BF Mid Category"
    }

    # Verify that required columns exist
    valid_rate_cols = [r for r in mapping if r in df.columns]
    if not valid_rate_cols:
        raise ValueError(f"‚ùå No expected rate columns found in {file_name}")

    long_frames = []

    # Build one melted DF per rate column
    for rate_col in valid_rate_cols:
        cat_col = mapping[rate_col]
        if cat_col not in df.columns:
            print(f"‚ö†Ô∏è Category column '{cat_col}' missing in {file_name}, skipping {rate_col}")
            continue

        temp_df = df[[tenor_col, rate_col, cat_col]].copy()
        temp_df.rename(columns={
            rate_col: "Rate",
            cat_col: "Category"
        }, inplace=True)
        temp_df["Type"] = rate_col
        long_frames.append(temp_df)

    if not long_frames:
        print(f"‚ö†Ô∏è No valid data extracted from {file_name}")
        return

    # Combine all melted sections
    long_df = pd.concat(long_frames, ignore_index=True)
    long_df["FileDate"] = extract_date_from_filename(file_name)

    # Save to Dataiku
    output_file_name = os.path.splitext(file_name)[0] + "_long.csv"
    buffer = io.StringIO()
    long_df.to_csv(buffer, index=False)
    buffer.seek(0)
    output_folder.upload_stream(output_file_name, io.BytesIO(buffer.getvalue().encode("utf-8")))

    print(f"‚úÖ {file_name}: {len(long_df)} rows processed (Date: {long_df['FileDate'].iloc[0]})")


# ------------------------------------------------------
# üöÄ MAIN EXECUTION
# ------------------------------------------------------
if use_dataiku:
    for path in input_folder.list_paths_in_partition():
        if path.lower().endswith(".csv"):
            file_name = os.path.basename(path)
            with input_folder.get_download_stream(path) as f:
                process_file_from_stream(f, file_name)
else:
    os.makedirs(output_folder_path, exist_ok=True)
    for filename in os.listdir(input_folder_path):
        if filename.lower().endswith(".csv"):
            with open(os.path.join(input_folder_path, filename), "rb") as f:
                process_file_from_stream(f, filename)
