# ============================================================
#   ENHANCED NOMINATION CLASSIFICATION SCRIPT
#   - Maintains exact dataset structure as specified
#   - Enhanced logging, validation, and error handling
#   - Comprehensive metrics and overlap checking
# ============================================================

import dataiku
import pandas as pd
import re
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime

# ------------------------------------------------------------
# CONFIGURATION
# ------------------------------------------------------------
CONFIG = {
    # Input dataset
    "input_dataset": "AccRelNo_AgainstNominationChecks",
    
    # Column names
    "nomination_column": "name",
    "relationship_column": "relationshipno",
    "account_column": "accountno",
    
    # Classification thresholds
    "min_name_length": 2,
    
    # Pattern configurations
    "patterns": {
        "ug_patterns": ["u/g", "u-g", "u\\g", "u g"],
        "sns_pattern": r"[A-Za-z]+\s+[A-Za-z]+\s+\d{2,}\s+[A-Za-z]+",
        "nom_ending_number": r"nom\w*\s*\d{2,}$",
        "digit_only": r"^\d+$",
    },
    
    # Word lists (expanded for better detection)
    "forbidden_words": [
        "sdu", "no held", "nomheld", "held", "nominee", 
        "nomination", "not held", "none", "null", "na", 
        "n/a", "nil", "blank", "no nominee", "nom not held"
    ],
    
    "relationship_words": [
        # Parents
        "mother", "mom", "mummy", "mamma", "ma", "mam", "mommy",
        "father", "dad", "daddy", "papa", "pappa", "pop", "dada",
        
        # Children
        "son", "daughter", "child", "children", "kid", "kids",
        "son/dtr", "son/daughter", "s/o", "d/o", "w/o",
        
        # Spouse
        "wife", "husband", "spouse", "partner",
        
        # Siblings
        "brother", "sister", "sibling", "sis", "bro", "bhai", "didi",
        
        # Extended family
        "uncle", "aunty", "aunt", "auntie", "nephew", "niece",
        "grandmother", "grandfather", "grandma", "grandpa", 
        "granny", "gramps", "nana", "granddad",
        
        # Guardians
        "guardian", "caretaker", "caregiver",
        
        # General
        "cousin", "relative", "family"
    ],
}

# ------------------------------------------------------------
# LOGGER SETUP
# ------------------------------------------------------------
class ClassificationLogger:
    def __init__(self):
        self.metrics = {
            "start_time": datetime.now(),
            "counts": {},
            "errors": [],
            "warnings": []
        }
    
    def log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")
        
        if level == "WARNING":
            self.metrics["warnings"].append(message)
        elif level == "ERROR":
            self.metrics["errors"].append(message)
    
    def record_count(self, category: str, count: int):
        self.metrics["counts"][category] = count
    
    def print_summary(self):
        total_records = sum(self.metrics["counts"].get(cat, 0) for cat in ["Success", "Empty", "TotalFailures"])
        duration = datetime.now() - self.metrics["start_time"]
        
        print("\n" + "="*60)
        print("CLASSIFICATION SUMMARY")
        print("="*60)
        print(f"Total processing time: {duration}")
        print(f"Total records processed: {total_records}")
        print(f"Warnings: {len(self.metrics['warnings'])}")
        print(f"Errors: {len(self.metrics['errors'])}")
        
        print("\nCategory Breakdown:")
        for category in ["Success", "Empty", "TotalFailures"]:
            count = self.metrics["counts"].get(category, 0)
            if total_records > 0:
                percentage = (count / total_records * 100)
                print(f"  {category}: {count} ({percentage:.1f}%)")
        
        # Show failure breakdown if available
        fail_categories = [
            "FailUG", "FailSNSPattern", "FailNOMOneWord", "FailForbiddenWords",
            "FailRelationship", "FailNOMEndingNumber", "FailOneWord", 
            "FailDigitOnly", "FailOthers"
        ]
        
        fail_counts = {cat: self.metrics["counts"].get(cat, 0) for cat in fail_categories}
        total_fails = sum(fail_counts.values())
        
        if total_fails > 0:
            print("\nFailure Breakdown:")
            for category in fail_categories:
                count = fail_counts[category]
                if count > 0:
                    percentage = (count / total_fails * 100)
                    print(f"  {category}: {count} ({percentage:.1f}%)")

# Initialize logger
logger = ClassificationLogger()

# ------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------
def clean_text(text: Any) -> str:
    """Clean and normalize text"""
    if text is None or pd.isna(text):
        return ""
    
    text = str(text).strip()
    # Replace multiple spaces, tabs, newlines with single space
    text = re.sub(r"\s+", " ", text)
    # Convert to lowercase for case-insensitive matching
    return text.lower()

def is_one_word_name(text: str) -> bool:
    """Check if text contains only one word"""
    text = text.strip()
    return len(text.split()) == 1 and len(text) >= CONFIG["min_name_length"]

def contains_any(text: str, word_list: List[str]) -> bool:
    """Check if text contains any of the words in the list"""
    text_lower = text.lower()
    return any(word.lower() in text_lower for word in word_list)

def validate_input_data(df: pd.DataFrame) -> bool:
    """Validate input data structure"""
    required_columns = [
        CONFIG["nomination_column"],
        CONFIG["relationship_column"],
        CONFIG["account_column"],
        "nominee_issues"
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.log(f"Missing required columns: {missing_columns}", "ERROR")
        return False
    
    # Check for unexpected nulls
    null_nom_count = df[CONFIG["nomination_column"]].isna().sum()
    if null_nom_count > 0:
        logger.log(f"Found {null_nom_count} null values in nomination column", "WARNING")
    
    return True

# ------------------------------------------------------------
# CLASSIFICATION LOGIC (PRIORITY ORDER)
# ------------------------------------------------------------
def classify_nomination(row: pd.Series) -> str:
    """
    Classify nomination with priority order:
    1. Success (no issues)
    2. Empty (missing/empty name)
    3. FailUG
    4. FailSNSPattern
    5. FailNOMOneWord
    6. FailForbiddenWords
    7. FailRelationship
    8. FailNOMEndingNumber
    9. FailOneWord
    10. FailDigitOnly
    11. FailOthers
    """
    try:
        # Extract data
        issues = row.get("nominee_issues", [])
        raw_name = str(row.get(CONFIG["nomination_column"], "")).strip()
        cleaned_name = clean_text(raw_name)
        
        # Handle non-list issues
        if not isinstance(issues, list):
            try:
                issues = list(issues) if issues else []
            except:
                issues = []
        
        # 1. Success - no issues
        if issues == []:
            return "Success"
        
        # 2. Empty - missing or empty name
        if issues == ["Missing or empty name"] or not cleaned_name:
            return "Empty"
        
        # Convert issues to lowercase for consistency
        issues_lower = [str(i).lower() for i in issues]
        
        # 3. FailUG - U/G cases
        if contains_any(cleaned_name, CONFIG["patterns"]["ug_patterns"]):
            return "FailUG"
        
        # 4. FailSNSPattern - String String Number String pattern
        if re.search(CONFIG["patterns"]["sns_pattern"], raw_name, re.IGNORECASE):
            return "FailSNSPattern"
        
        # 5. FailNOMOneWord - Contains "NOM" and is single word
        if "nom" in cleaned_name and is_one_word_name(raw_name):
            return "FailNOMOneWord"
        
        # 6. FailForbiddenWords - Contains forbidden words
        if contains_any(cleaned_name, CONFIG["forbidden_words"]):
            return "FailForbiddenWords"
        
        # 7. FailRelationship - Contains relationship words
        if contains_any(cleaned_name, CONFIG["relationship_words"]):
            return "FailRelationship"
        
        # 8. FailNOMEndingNumber - NOM ending with numbers
        if re.search(CONFIG["patterns"]["nom_ending_number"], cleaned_name):
            return "FailNOMEndingNumber"
        
        # 9. FailOneWord - Single word names
        if is_one_word_name(raw_name):
            return "FailOneWord"
        
        # 10. FailDigitOnly - Digit-only names
        if re.match(CONFIG["patterns"]["digit_only"], raw_name):
            return "FailDigitOnly"
        
        # 11. FailOthers - Catch-all for remaining cases
        return "FailOthers"
        
    except Exception as e:
        error_msg = f"Classification error for row: {str(e)}"
        logger.log(error_msg, "ERROR")
        # Return FailOthers as safe default
        return "FailOthers"

# ------------------------------------------------------------
# MAIN PROCESSING FUNCTION
# ------------------------------------------------------------
def process_nomination_classification():
    """Main processing function"""
    logger.log("Starting nomination classification process")
    
    # --------------------------------------------------------
    # READ DATASET
    # --------------------------------------------------------
    try:
        df = dataiku.Dataset(CONFIG["input_dataset"]).get_dataframe()
        logger.log(f"Loaded dataset with {len(df)} rows")
    except Exception as e:
        logger.log(f"Failed to load dataset: {e}", "ERROR")
        return
    
    # --------------------------------------------------------
    # VALIDATE INPUT DATA
    # --------------------------------------------------------
    if not validate_input_data(df):
        logger.log("Input data validation failed. Exiting.", "ERROR")
        return
    
    # --------------------------------------------------------
    # PREPARE DATA
    # --------------------------------------------------------
    df["name_clean"] = df[CONFIG["nomination_column"]].apply(clean_text)
    
    # --------------------------------------------------------
    # APPLY CLASSIFICATION
    # --------------------------------------------------------
    logger.log("Applying classification rules...")
    df["NominationCase"] = df.apply(classify_nomination, axis=1)
    
    # --------------------------------------------------------
    # SPLIT INTO DATASETS (AS SPECIFIED)
    # --------------------------------------------------------
    logger.log("Splitting data into specified datasets...")
    
    # 1️⃣ Independent top-level datasets
    success_df = df[df["NominationCase"] == "Success"].copy()
    empty_df = df[df["NominationCase"] == "Empty"].copy()
    
    # 2️⃣ All failed cases combined
    fail_mask = df["NominationCase"].str.startswith("Fail")
    fail_all_df = df[fail_mask].copy()
    
    # Record counts
    logger.record_count("Success", len(success_df))
    logger.record_count("Empty", len(empty_df))
    logger.record_count("TotalFailures", len(fail_all_df))
    
    # --------------------------------------------------------
    # WRITE OUTPUT DATASETS
    # --------------------------------------------------------
    logger.log("Writing output datasets...")
    
    # Write top-level datasets
    dataiku.Dataset("SuccessNomination").write_with_schema(success_df)
    dataiku.Dataset("EmptyNomination").write_with_schema(empty_df)
    dataiku.Dataset("FailAllNomination").write_with_schema(fail_all_df)
    
    # 3️⃣ Mutually exclusive fail category datasets
    fail_categories = [
        "FailUG", "FailSNSPattern", "FailNOMOneWord", "FailForbiddenWords",
        "FailRelationship", "FailNOMEndingNumber", "FailOneWord", 
        "FailDigitOnly", "FailOthers"
    ]
    
    for category in fail_categories:
        subset = fail_all_df[fail_all_df["NominationCase"] == category].copy()
        dataset_name = f"{category}Nomination"
        
        if not subset.empty:
            dataiku.Dataset(dataset_name).write_with_schema(subset)
            logger.record_count(category, len(subset))
        else:
            logger.record_count(category, 0)
    
    # --------------------------------------------------------
    # VALIDATION AND OVERLAP CHECKS
    # --------------------------------------------------------
    logger.log("Performing validation checks...")
    
    # Check 1: Ensure all rows are accounted for
    total_classified = len(success_df) + len(empty_df) + len(fail_all_df)
    if total_classified != len(df):
        missing = len(df) - total_classified
        logger.log(f"WARNING: {missing} rows not classified!", "WARNING")
    
    # Check 2: Ensure no overlap between Success, Empty, and Fail
    rel_col = CONFIG["relationship_column"]
    acc_col = CONFIG["account_column"]
    
    success_keys = set(zip(success_df[rel_col], success_df[acc_col]))
    empty_keys = set(zip(empty_df[rel_col], empty_df[acc_col]))
    fail_keys = set(zip(fail_all_df[rel_col], fail_all_df[acc_col]))
    
    # Check overlaps between different categories
    success_empty_overlap = success_keys.intersection(empty_keys)
    success_fail_overlap = success_keys.intersection(fail_keys)
    empty_fail_overlap = empty_keys.intersection(fail_keys)
    
    if success_empty_overlap:
        logger.log(f"WARNING: {len(success_empty_overlap)} overlaps between Success and Empty", "WARNING")
    
    if success_fail_overlap:
        logger.log(f"WARNING: {len(success_fail_overlap)} overlaps between Success and Fail", "WARNING")
    
    if empty_fail_overlap:
        logger.log(f"WARNING: {len(empty_fail_overlap)} overlaps between Empty and Fail", "WARNING")
    
    # Check 3: Ensure fail categories are mutually exclusive
    fail_category_dfs = []
    for category in fail_categories:
        subset = fail_all_df[fail_all_df["NominationCase"] == category]
        fail_category_dfs.append(subset)
    
    if fail_category_dfs:
        # Combine all fail category subsets
        combined_fails = pd.concat(fail_category_dfs)
        
        # Check for duplicates across categories
        overlap_check = combined_fails.groupby([rel_col, acc_col]).size()
        overlaps = overlap_check[overlap_check > 1]
        
        if not overlaps.empty:
            logger.log(f"WARNING: {len(overlaps)} overlaps detected across fail categories", "WARNING")
            logger.log("Sample of overlapping records:", "WARNING")
            
            # Get sample of overlapping records
            overlap_samples = []
            for (rel, acc), count in overlaps.head(3).items():
                records = combined_fails[(combined_fails[rel_col] == rel) & 
                                        (combined_fails[acc_col] == acc)]
                categories = records["NominationCase"].unique()
                overlap_samples.append(f"  Rel={rel}, Acc={acc}: {count} entries in {categories}")
            
            for sample in overlap_samples:
                logger.log(sample, "WARNING")
        else:
            logger.log("✅ All fail categories are mutually exclusive (no overlaps)")
    
    # Check 4: Verify all fail rows are in exactly one category
    total_fail_by_category = sum(len(fail_all_df[fail_all_df["NominationCase"] == cat]) for cat in fail_categories)
    if total_fail_by_category != len(fail_all_df):
        missing_fails = len(fail_all_df) - total_fail_by_category
        logger.log(f"WARNING: {missing_fails} fail rows not assigned to any specific category", "WARNING")
    
    # --------------------------------------------------------
    # DATA QUALITY METRICS
    # --------------------------------------------------------
    logger.log("\n" + "="*60)
    logger.log("DATA QUALITY METRICS")
    logger.log("="*60)
    
    # Calculate percentages
    total_records = len(df)
    success_pct = (len(success_df) / total_records * 100) if total_records > 0 else 0
    empty_pct = (len(empty_df) / total_records * 100) if total_records > 0 else 0
    fail_pct = (len(fail_all_df) / total_records * 100) if total_records > 0 else 0
    
    logger.log(f"Total Records: {total_records}")
    logger.log(f"Success Nominations: {len(success_df)} ({success_pct:.1f}%)")
    logger.log(f"Empty Nominations: {len(empty_df)} ({empty_pct:.1f}%)")
    logger.log(f"Failed Nominations: {len(fail_all_df)} ({fail_pct:.1f}%)")
    
    if len(fail_all_df) > 0:
        logger.log("\nTop Failure Categories:")
        fail_breakdown = fail_all_df["NominationCase"].value_counts().head(5)
        for case, count in fail_breakdown.items():
            pct = (count / len(fail_all_df)) * 100
            logger.log(f"  {case}: {count} ({pct:.1f}%)")
    
    # --------------------------------------------------------
    # CREATE AUDIT LOG
    # --------------------------------------------------------
    try:
        audit_data = []
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Add summary stats
        audit_data.append({
            "Timestamp": timestamp,
            "Category": "SUMMARY",
            "Metric": "Total Records",
            "Value": str(total_records),
            "Details": "Input dataset row count"
        })
        
        audit_data.append({
            "Timestamp": timestamp,
            "Category": "SUMMARY",
            "Metric": "Success Rate",
            "Value": f"{success_pct:.1f}%",
            "Details": f"{len(success_df)} successful nominations"
        })
        
        # Add counts for each category
        for category in ["Success", "Empty"] + fail_categories:
            count = len(df[df["NominationCase"] == category])
            audit_data.append({
                "Timestamp": timestamp,
                "Category": "CLASSIFICATION",
                "Metric": category,
                "Value": str(count),
                "Details": f"Rows classified as {category}"
            })
        
        # Add warnings/errors
        for i, warning in enumerate(self.metrics["warnings"][:10]):  # Limit to 10 warnings
            audit_data.append({
                "Timestamp": timestamp,
                "Category": "WARNING",
                "Metric": f"Warning_{i+1}",
                "Value": "⚠",
                "Details": warning[:200]  # Truncate long messages
            })
        
        audit_df = pd.DataFrame(audit_data)
        dataiku.Dataset("NominationClassificationAudit").write_with_schema(audit_df)
        logger.log("Audit log created successfully")
        
    except Exception as e:
        logger.log(f"Could not create audit log: {e}", "WARNING")
    
    # --------------------------------------------------------
    # FINAL SUMMARY
    # --------------------------------------------------------
    logger.log("\n" + "="*60)
    logger.log("PROCESS COMPLETED SUCCESSFULLY")
    logger.log("="*60)
    logger.log(f"Output datasets created:")
    logger.log(f"  1. SuccessNomination")
    logger.log(f"  2. EmptyNomination")
    logger.log(f"  3. FailAllNomination")
    for category in fail_categories:
        logger.log(f"  4. {category}Nomination")
    
    # Print final summary
    logger.print_summary()

# ------------------------------------------------------------
# EXECUTION
# ------------------------------------------------------------
if __name__ == "__main__":
    try:
        process_nomination_classification()
        logger.log("\nScript execution completed successfully!")
    except KeyboardInterrupt:
        logger.log("\nScript interrupted by user", "WARNING")
    except Exception as e:
        logger.log(f"\nUnexpected error in main execution: {e}", "ERROR")
        # Re-raise to see full traceback in Dataiku logs
        raise
