import re
import os
import sys
import shutil
import tempfile
import pandas as pd
import dataiku
from openpyxl import load_workbook

SHEET_NAME_PATTERN = r'COMPUTED[_ ]?DATA'

# -----------------------------
# Fixed Dataiku Folder Handles
# -----------------------------
input_folder = dataiku.Folder("DCfYhFp0")      # 📂 Replace with your input folder ID
output_folder_tables = dataiku.Folder("GwwHsyzO")  # 📂 Replace with your output folder ID for tables
output_folder_summary = dataiku.Folder("k3NfXo0Q") # 📂 Replace with your output folder ID for summary CSV
# -----------------------------
# Logging Helper
# -----------------------------
def log(message):
    print(f"[LOG] {message}", file=sys.stderr, flush=True)

# -----------------------------
# Utility Functions
# -----------------------------
def extract_date_from_filename(filename):
    """Extract date pattern like 01-Aug-25"""
    match = re.search(r'\d{2}-[A-Za-z]{3}-\d{2}', filename)
    return match.group(0) if match else ""


def read_excel_tab(filepath):
    """Read Origami table and hardcode column renames"""
    wb = load_workbook(filepath, data_only=True)
    sheet_name = None

    # Find the sheet
    for name in wb.sheetnames:
        if re.search(SHEET_NAME_PATTERN, name, re.IGNORECASE):
            sheet_name = name
            break

    if not sheet_name:
        log("⚠️ 'COMPUTED DATA' sheet not found in workbook.")
        return None, None

    ws = wb[sheet_name]
    all_rows = [list(r) for r in ws.iter_rows(values_only=True) if any(r)]

    # Header rows (row 2 and 3)
    header_row_1 = [str(x).strip() if x else "" for x in all_rows[1]]
    header_row_2 = [str(x).strip() if x else "" for x in all_rows[2]]

    # Flatten header
    combined_headers = []
    for h1, h2 in zip(header_row_1, header_row_2):
        if h1 and h2:
            combined_headers.append(f"{h1} {h2}".strip())
        elif h1:
            combined_headers.append(h1.strip())
        elif h2:
            combined_headers.append(h2.strip())
        else:
            combined_headers.append("")

    # Create dataframe from row 4 onward
    df_table = pd.DataFrame(all_rows[3:], columns=combined_headers)

    # -------------------------------
    # HARD-CODED RENAMES
    # -------------------------------
    rename_map = {}

    # 3rd to 6th column -> DF-1 section
    df1_names = ["DF-1 AAA", "DF-1 AA+", "DF-1 AA", "DF-1 AA-"]
    for idx, name in zip(range(2, 6), df1_names):
        rename_map[df_table.columns[idx]] = name

    # Columns after DF-5 -> Final section (next 8 columns)
    final_names = [
        "Final AAA", "Final AAA Source",
        "Final AA+", "Final AA+ Source",
        "Final AA", "Final AA Source",
        "Final AA-", "Final AA- Source"
    ]
    # Assuming DF-5 is the 12th column (index 11)
    for idx, name in zip(range(12, 20), final_names):
        if idx < len(df_table.columns):
            rename_map[df_table.columns[idx]] = name

    df_table.rename(columns=rename_map, inplace=True)

    # -------------------------------
    # Extract summary rows (maker-checker metadata)
    # -------------------------------
    summary_rows = all_rows[3+len(df_table):]  # rows after the table
    summary_texts = [" ".join([str(c) for c in row if c]).strip() for row in summary_rows if any(row)]
    metadata = []
    current = {}

    for line in summary_texts:
        line = line.strip()
        if re.search(r'\bUSER\b', line):
            if current:
                metadata.append(current)
                current = {}
            match = re.search(r'USER[:\s]*(.*)', line)
            current["USER"] = match.group(1).strip() if match else ""
        elif re.search(r'\bBenchmark\b', line):
            match = re.search(r'Benchmark[:\s]*(.*)', line)
            current["Benchmark"] = match.group(1).strip() if match else ""
        elif re.search(r'\bDate\b', line):
            match = re.search(r'Date[:\s]*(.*)', line)
            current["Date (in file)"] = match.group(1).strip() if match else ""
        elif re.search(r'\bStatus\b', line) and "Status Time" not in line:
            match = re.search(r'Status[:\s]*(.*)', line)
            current["Status"] = match.group(1).strip() if match else ""
        elif re.search(r'\bStatus Time\b', line):
            match = re.search(r'Status Time[:\s]*(.*)', line)
            current["Status Time"] = match.group(1).strip() if match else ""

    if current:
        metadata.append(current)

    return df_table, metadata

# -----------------------------
# Main Processing
# -----------------------------
def main():
    try:
        log("🔍 Scanning top-level of input folder for ORIGAMI Excel files...")

        all_files = input_folder.list_paths_in_partition()
        excel_files = [
            f for f in all_files
            if "/" not in f.strip("/")  # ensures top-level only
            and os.path.basename(f).startswith("ORIGAMI")
            and f.lower().endswith((".xlsx", ".xls"))
        ]

        if not excel_files:
            raise Exception("❌ No ORIGAMI Excel files found in the top level of the input folder.")

        summary_records = []

        for path in excel_files:
            file_name = os.path.basename(path)
            log(f"📄 Processing: {file_name}")

            # Download to temporary file
            with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as tmp_file:
                tmp_file_path = tmp_file.name
                with input_folder.get_download_stream(path) as stream:
                    shutil.copyfileobj(stream, tmp_file)

            # Extract date from filename
            file_date = extract_date_from_filename(file_name)

            # Parse Excel
            df_table, metadata_list = read_excel_tab(tmp_file_path)
            os.remove(tmp_file_path)

            if df_table is None:
                log(f"⚠️ Skipped {file_name}: No valid 'Excel Tab' or table found.")
                continue

            # Save main table to output folder
            with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as tmp_out:
                df_table.to_excel(tmp_out.name, index=False)
                out_name = file_name.replace(".xlsx", "_table.xlsx")
                with open(tmp_out.name, "rb") as f_out:
                    output_folder_tables.upload_stream(out_name, f_out)
                os.remove(tmp_out.name)
                log(f"✅ Saved table: {out_name}")

            # Add metadata info to summary
            if metadata_list:
                for meta in metadata_list:
                    summary_records.append({
                        "FileName": file_name,
                        "FileDate": file_date,
                        "USER": meta.get("USER", ""),
                        "Benchmark": meta.get("Benchmark", ""),
                        "Date (in file)": meta.get("Date (in file)", ""),
                        "Status": meta.get("Status", ""),
                        "Status Time": meta.get("Status Time", "")
                    })
            else:
                summary_records.append({
                    "FileName": file_name,
                    "FileDate": file_date,
                    "USER": "", "Benchmark": "", "Date (in file)": "",
                    "Status": "", "Status Time": ""
                })

        # Save summary CSV
        if summary_records:
            df_summary = pd.DataFrame(summary_records)
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8', newline='') as tmp_csv:
                df_summary.to_csv(tmp_csv.name, index=False)
                csv_name = "maker_checker_summary.csv"
                with open(tmp_csv.name, 'rb') as f:
                    output_folder_summary.upload_stream(csv_name, f)
                os.remove(tmp_csv.name)
            log(f"✅ Summary written to {csv_name} ({len(summary_records)} rows)")
        else:
            log("⚠️ No metadata found to include in summary.")

    except Exception as e:
        log(f"🔥 ERROR: {str(e)}")
        raise

# -----------------------------
# Entrypoint
# -----------------------------
if __name__ == "__main__":
    main()
