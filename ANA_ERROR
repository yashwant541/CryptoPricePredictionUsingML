import pandas as pd
import os
import io
import re
import dataiku

# ------------------------------------------------------
# ‚öôÔ∏è CONFIGURATION
# ------------------------------------------------------
use_dataiku = True  # üîÑ Switch between Dataiku and local

if use_dataiku:
    input_folder = dataiku.Folder("XXXXXXX")       # Input: folder containing matchflag_reason files
    output_folder = dataiku.Folder("XXXXXXX")      # Output: folder for consolidated mismatch summaries
else:
    input_folder_path = r"C:\path\to\input"
    output_folder_path = r"C:\path\to\output"

# ------------------------------------------------------
# üß© Helper functions
# ------------------------------------------------------

def extract_file_date(filename):
    """
    Extracts the file date assuming it's between the 3rd and 4th underscore,
    e.g. FX_Option_Match_13-Dec-2025_Results.csv ‚Üí 13-Dec-2025
    """
    parts = filename.split('_')
    if len(parts) >= 4:
        possible_date = parts[3]
        possible_date = re.sub(r'\.csv$', '', possible_date, flags=re.IGNORECASE)
        return possible_date.strip()
    return None


def process_file(file_path, file_name):
    """
    Reads the match result CSV, finds mismatches,
    extracts date from filename, and returns DataFrame
    with FileName, FileDate, Category, and Mismatch Reason.
    """
    df = pd.read_csv(file_path)

    file_date = extract_file_date(file_name)

    # --- Identify mismatches ---
    if "Match_Flag" in df.columns:
        mismatches = df[df["Match_Flag"] == False].copy()
        mismatches["Mismatch Reason"] = mismatches.get("Mismatch_Reason", "Match_Flag is False")
    elif "Match_Result" in df.columns:
        mismatches = df[df["Match_Result"].astype(str).str.lower() != "match found"].copy()
        mismatches["Mismatch Reason"] = "Match_Result not 'Match Found'"
    else:
        return pd.DataFrame(columns=["FileName", "FileDate", "Category", "Mismatch Reason"])

    # --- Ensure essential columns ---
    if "Category" not in mismatches.columns:
        mismatches["Category"] = "Unknown"

    # --- Add metadata ---
    mismatches["FileName"] = file_name
    mismatches["FileDate"] = file_date

    return mismatches[["FileName", "FileDate", "Category", "Mismatch Reason"]]


# ------------------------------------------------------
# üöÄ Main Execution
# ------------------------------------------------------

def consolidate_mismatches():
    mismatch_records = []

    if use_dataiku:
        # --- Read all CSVs from Dataiku folder ---
        for path in input_folder.list_paths_in_partition():
            if path.endswith(".csv"):
                file_name = os.path.basename(path)
                with input_folder.get_download_stream(path) as f:
                    df = pd.read_csv(f)
                    temp_path = f"/tmp/{file_name}"
                    df.to_csv(temp_path, index=False)
                    result_df = process_file(temp_path, file_name)
                    if not result_df.empty:
                        mismatch_records.append(result_df)
    else:
        for filename in os.listdir(input_folder_path):
            if filename.endswith(".csv"):
                result_df = process_file(os.path.join(input_folder_path, filename), filename)
                if not result_df.empty:
                    mismatch_records.append(result_df)

    # --- Combine all mismatches ---
    if mismatch_records:
        detailed_df = pd.concat(mismatch_records, ignore_index=True)
    else:
        detailed_df = pd.DataFrame(columns=["FileName", "FileDate", "Category", "Mismatch Reason"])

    # --- Summary by File + Date + Category ---
    summary_df = (
        detailed_df.groupby(["FileName", "FileDate", "Category"])
        .size()
        .reset_index(name="Mismatch_Count")
    )

    # --- Total mismatches per file/date ---
    total_summary = (
        detailed_df.groupby(["FileName", "FileDate"])
        .size()
        .reset_index(name="Total_Mismatch_Count")
    )

    # --- Save outputs ---
    if use_dataiku:
        # Detailed output
        buffer = io.StringIO()
        detailed_df.to_csv(buffer, index=False)
        output_folder.upload_stream("consolidated_mismatches_detailed.csv", io.BytesIO(buffer.getvalue().encode("utf-8")))

        # Summary output
        buffer = io.StringIO()
        summary_df.to_csv(buffer, index=False)
        output_folder.upload_stream("consolidated_mismatches_summary.csv", io.BytesIO(buffer.getvalue().encode("utf-8")))

        # Total summary
        buffer = io.StringIO()
        total_summary.to_csv(buffer, index=False)
        output_folder.upload_stream("consolidated_total_summary.csv", io.BytesIO(buffer.getvalue().encode("utf-8")))

    else:
        os.makedirs(output_folder_path, exist_ok=True)
        detailed_df.to_csv(os.path.join(output_folder_path, "consolidated_mismatches_detailed.csv"), index=False)
        summary_df.to_csv(os.path.join(output_folder_path, "consolidated_mismatches_summary.csv"), index=False)
        total_summary.to_csv(os.path.join(output_folder_path, "consolidated_total_summary.csv"), index=False)

    print(f"‚úÖ Consolidation complete ‚Üí {len(detailed_df)} detailed rows, {len(summary_df)} summaries.")


# ------------------------------------------------------
# ‚ñ∂Ô∏è Run Script
# ------------------------------------------------------
if __name__ == "__main__":
    consolidate_mismatches()
