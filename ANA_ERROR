import os
import pandas as pd

# === USER CONFIGURATION ===
use_dataiku = False  # True = Dataiku mode, False = local mode

# Local paths
input_folder = r"C:\path\to\txt_files"
output_folder = r"C:\path\to\output_folder"

# Dataiku folders
dataiku_input_name = "input_folder"
dataiku_output_name = "output_folder"

# Keywords to extract
keywords = ["AVERAGE ATM VOL", "TEXT B", "TEXT C"]

# ===========================

if use_dataiku:
    import dataiku
    input_folder_handle = dataiku.Folder(dataiku_input_name)
    output_folder_handle = dataiku.Folder(dataiku_output_name)

def extract_keyword_values_from_line(line, keyword):
    parts = line.strip().split(keyword, 1)
    if len(parts) > 1:
        values_part = parts[1].strip()
        values = [v for v in values_part.split() if v.strip()]
        return values
    return None

def process_single_file(file_path, keywords, folder_handle=None):
    # Read file lines
    if folder_handle:
        with folder_handle.get_download_stream(file_path) as f:
            lines = f.read().decode("utf-8", errors="ignore").splitlines()
    else:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
    
    extracted_data = []
    for keyword in keywords:
        for line in lines:
            if keyword in line:
                values = extract_keyword_values_from_line(line, keyword)
                if values:
                    extracted_data.append([keyword] + values)
                break
    return extracted_data

# === MAIN EXECUTION ===
if __name__ == "__main__":
    print("ğŸš€ Starting keyword extraction per file...")

    if use_dataiku:
        file_list = input_folder_handle.list_paths_in_partition()
        txt_files = [fp for fp in file_list if fp.lower().endswith(".txt")]
        print(f"ğŸ“ Found {len(txt_files)} TXT files in Dataiku folder.")

        for file_path in txt_files:
            data = process_single_file(file_path, keywords, folder_handle=input_folder_handle)
            if not data:
                print(f"âš ï¸ No keywords found in {file_path}. Skipping.")
                continue
            
            max_len = max(len(row) for row in data)
            columns = ["Keyword"] + [f"Val{i}" for i in range(1, max_len)]
            df = pd.DataFrame(data, columns=columns)

            # Save with original file name
            output_path = os.path.join("/tmp", os.path.basename(file_path).replace(".txt", ".xlsx"))
            df.to_excel(output_path, index=False)
            with open(output_path, "rb") as f:
                output_folder_handle.upload_stream(os.path.basename(file_path).replace(".txt", ".xlsx"), f)
            print(f"âœ… Processed {file_path} -> uploaded {os.path.basename(output_path)}")

    else:
        txt_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.lower().endswith(".txt")]
        print(f"ğŸ“ Found {len(txt_files)} TXT files in local folder.")

        for file_path in txt_files:
            data = process_single_file(file_path, keywords)
            if not data:
                print(f"âš ï¸ No keywords found in {file_path}. Skipping.")
                continue
            
            max_len = max(len(row) for row in data)
            columns = ["Keyword"] + [f"Val{i}" for i in range(1, max_len)]
            df = pd.DataFrame(data, columns=columns)

            # Save with original file name
            output_file_path = os.path.join(output_folder, os.path.basename(file_path).replace(".txt", ".xlsx"))
            df.to_excel(output_file_path, index=False)
            print(f"âœ… Processed {file_path} -> saved {output_file_path}")
