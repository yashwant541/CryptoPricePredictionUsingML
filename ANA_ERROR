import pandas as pd
from pathlib import Path
import re
from collections import defaultdict

def load_stopwords(stopwords_file):
    """Load stopwords from a file (one word per line)"""
    try:
        with open(stopwords_file, 'r') as f:
            return set(line.strip().lower() for line in f if line.strip())
    except FileNotFoundError:
        print(f"Stopwords file '{stopwords_file}' not found. Using default stopwords.")
        return {'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what', 
                'where', 'from', 'how', 'of', 'for', 'to', 'at', 'in', 'on', 'that'}

def get_file_columns(file_path):
    """Show columns in file and let user select one"""
    try:
        df = pd.read_excel(file_path)
        print("\nColumns in file:")
        for i, col in enumerate(df.columns, 1):
            print(f"{i}. {col}")
        
        while True:
            try:
                col_num = int(input("Enter column number to use: ")) - 1
                if 0 <= col_num < len(df.columns):
                    return df.columns[col_num]
                print("Invalid column number. Try again.")
            except ValueError:
                print("Please enter a valid number.")
    except Exception as e:
        print(f"Error reading file: {e}")
        return None

def tokenize_text(text):
    """Tokenize text into words while preserving original structure"""
    return re.findall(r"\b[\w'-]+\b", str(text).lower())

def find_matches(text, keywords, stopwords):
    """Find matches in text using exploded words internally"""
    words = tokenize_text(text)
    matches = set()
    found_stopwords = set()
    
    # First pass: exact matches after stopword removal
    clean_words = []
    for word in words:
        if word in stopwords:
            found_stopwords.add(word)
            continue
        clean_words.append(word)
        if word in keywords:
            matches.add(word)
    
    # Second pass: partial matches in cleaned text
    clean_text = ' '.join(clean_words)
    for keyword in keywords:
        if len(keyword) > 2 and keyword in clean_text:
            if keyword not in matches:  # Only add if not already an exact match
                matches.add(f"{keyword} (Partial)")
    
    return sorted(matches), sorted(found_stopwords)

def main():
    print("Sentence-Aware Keyword Matching Tool")
    print("-----------------------------------")
    
    # Get stopwords file
    stopwords_file = input("Enter path to stopwords file (or press Enter for default): ")
    stopwords = load_stopwords(stopwords_file) if stopwords_file else load_stopwords('')
    
    # Get input files
    file1 = input("\nEnter path to main data file: ")
    if not Path(file1).exists():
        print("File not found!")
        return
    
    col1 = get_file_columns(file1)
    if not col1:
        return
    
    file2 = input("\nEnter path to keywords file: ")
    if not Path(file2).exists():
        print("File not found!")
        return
    
    col2 = get_file_columns(file2)
    if not col2:
        return
    
    try:
        # Read files
        df_main = pd.read_excel(file1)
        df_keywords = pd.read_excel(file2)
        
        # Prepare keywords (remove stopwords and empty values)
        keywords = set(str(word).strip().lower() 
                      for word in df_keywords[col2].dropna().unique() 
                      if str(word).strip())
        keywords = keywords - stopwords
        
        # Process data
        results = []
        stopwords_report = defaultdict(int)
        
        for idx, row in df_main.iterrows():
            text = row[col1]
            matches, found_stopwords = find_matches(text, keywords, stopwords)
            
            # Update stopwords frequency
            for sw in found_stopwords:
                stopwords_report[sw] += 1
            
            results.append({
                'Match': "TRUE" if matches else "FALSE",
                'Matched_Keywords': ", ".join(matches) if matches else "",
                'StopWords_Found': ", ".join(found_stopwords) if found_stopwords else "",
                'Match_Count': len(matches),
                'StopWord_Count': len(found_stopwords)
            })
        
        # Create output DataFrame (preserving original structure)
        output_df = pd.concat([
            df_main,
            pd.DataFrame(results)
        ], axis=1)
        
        # Create stopwords frequency report
        df_stopwords = pd.DataFrame(
            sorted(stopwords_report.items(), key=lambda x: x[1], reverse=True),
            columns=['StopWord', 'Frequency']
        )
        
        # Save outputs
        base_name = input("\nEnter base name for output files (e.g., 'results'): ")
        
        output_file = f"{base_name}_matched.xlsx"
        stopwords_file = f"{base_name}_stopwords.xlsx"
        
        output_df.to_excel(output_file, index=False)
        df_stopwords.to_excel(stopwords_file, index=False)
        
        print(f"\nMain results saved to: {output_file}")
        print(f"Stopwords report saved to: {stopwords_file}")
        
        # Show summary
        total_matches = output_df['Match'].value_counts().get('TRUE', 0)
        unique_stopwords = len(df_stopwords)
        avg_matches = output_df['Match_Count'].mean()
        
        print("\nSummary Statistics:")
        print(f"- Total records processed: {len(output_df)}")
        print(f"- Records with matches: {total_matches}")
        print(f"- Average matches per record: {avg_matches:.1f}")
        print(f"- Unique stopwords found: {unique_stopwords}")
        
    except Exception as e:
        print(f"\nError: {e}")

if __name__ == "__main__":
    main()
