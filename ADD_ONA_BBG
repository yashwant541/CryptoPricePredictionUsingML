import pdfplumber
import pandas as pd
import os
import dataiku
import tempfile
import shutil
import re

# Folder handles
input_folder = dataiku.Folder("input_pdf_folder")     # üîÅ Replace with your input folder ID
output_folder = dataiku.Folder("output_csv_folder")   # üîÅ Replace with your output folder ID

# Helper to detect date-like entries
def contains_dates(row):
    if not row:
        return False
    date_pattern = re.compile(r'\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b')  # matches 31.03.2025 or 31-12-24
    return any(cell and date_pattern.search(cell) for cell in row)

# List PDF files
pdf_files = input_folder.list_paths_in_partition()
pdf_files = [f for f in pdf_files if f.lower().endswith(".pdf")]

if not pdf_files:
    raise Exception("‚ùå No PDF files found in the input folder.")

# Process each PDF
for pdf_path in pdf_files:
    file_name = os.path.basename(pdf_path)

    # Save PDF temporarily
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_pdf:
        tmp_pdf_path = tmp_pdf.name
        with input_folder.get_download_stream(pdf_path) as stream:
            shutil.copyfileobj(stream, tmp_pdf)

    with pdfplumber.open(tmp_pdf_path) as pdf:
        table_count = 0
        for page_num, page in enumerate(pdf.pages, start=1):
            # Table extraction with tuned settings
            settings = {
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "snap_tolerance": 3,
                "intersection_tolerance": 3,
                "edge_min_length": 3,
                "join_tolerance": 2,
                "min_words_vertical": 1,
                "min_words_horizontal": 1,
                "keep_blank_chars": True
            }

            tables = page.extract_tables(table_settings=settings)

            for idx, table in enumerate(tables, start=1):
                if not table or len(table) < 2:
                    continue

                # Try to detect correct header row
                header_row = None
                data_start_index = 1  # Default to skip first row as header

                for i in range(min(4, len(table))):  # Look in first 4 rows
                    if contains_dates(table[i]):
                        header_row = table[i]
                        data_start_index = i + 1
                        break

                if not header_row:
                    header_row = table[0]
                    data_start_index = 1

                df = pd.DataFrame(table[data_start_index:], columns=header_row)
                table_count += 1

                # Save to temp CSV
                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, newline='', encoding='utf-8') as tmp_csv:
                    df.to_csv(tmp_csv, index=False)
                    csv_temp_path = tmp_csv.name

                # Upload to Dataiku
                output_csv_name = f"{os.path.splitext(file_name)[0]}_page{page_num}_table{idx}.csv"
                with open(csv_temp_path, 'rb') as f:
                    output_folder.upload_stream(output_csv_name, f)

                os.remove(csv_temp_path)

    os.remove(tmp_pdf_path)

print("‚úÖ All tables extracted with intelligent header detection.")
