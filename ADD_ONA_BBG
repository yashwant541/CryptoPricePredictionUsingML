import numpy as np
np.int = int
np.float = float

import nltk
import re
import csv
import sys
import os
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from dateutil.parser import parse as parse_date

# Setup NLTK path (optional: update as needed)
nltk.data.path.append(r'C:\Users\2011747\nltk_library')

# Ensure stopwords are available
try:
    stop_words = set(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')
    stop_words = set(stopwords.words('english'))

def split_emails(raw_text):
    parts = re.split(r"(?=^From: )", raw_text, flags=re.IGNORECASE | re.MULTILINE)
    if not parts[0].strip().lower().startswith("from:"):
        first = parts.pop(0)
        parts = [first] + parts
    return parts

def extract_field(email, field):
    # Handles multiline fields until next known header or end
    pattern = rf"{field}:(.*?)(?=\n\S+:|\Z)"
    match = re.search(pattern, email, re.IGNORECASE | re.DOTALL)
    return match.group(1).strip().replace('\n', ' ') if match else ""

def parse_date_time(date_str):
    if not date_str:
        return "", ""
    try:
        dt = parse_date(date_str.strip(), fuzzy=True)
        return dt.date().isoformat(), dt.time().isoformat()
    except:
        return date_str.strip(), ""

def extract_body(email):
    split_point = re.search(r"\n\s*\n", email)
    if split_point:
        return email[split_point.end():].strip()
    return email.strip()

def tokenize_body(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    words = word_tokenize(text)
    cleaned = [w for w in words if w not in stop_words and w.isalpha()]
    return cleaned

def parse_email_chain(text):
    email_chunks = split_emails(text)
    parsed = []
    for i, email in enumerate(email_chunks):
        sender = extract_field(email, "From")
        receiver = extract_field(email, "To")
        cc = extract_field(email, "Cc")
        bcc = extract_field(email, "Bcc")
        subject = extract_field(email, "Subject")
        date_raw = extract_field(email, "Sent")

        date, time = parse_date_time(date_raw)
        body = extract_body(email)
        tokens = tokenize_body(body)

        parsed.append({
            "Email Sequence": i + 1,
            "Sender": sender,
            "Receiver": receiver,
            "cc": cc,
            "bcc": bcc,
            "subject": subject,
            "email body": body,
            "tokens": ", ".join(tokens),
            "date": date,
            "time": time
        })
    return parsed

def save_to_csv(parsed_emails, output_file="parsed_emails.csv"):
    if not parsed_emails:
        print("‚ùå No emails found.")
        return
    fieldnames = parsed_emails[0].keys()
    with open(output_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in parsed_emails:
            writer.writerow(row)
    print(f"‚úÖ CSV saved: {output_file}")

def save_summary(parsed_emails, summary_file="summary_output.txt"):
    with open(summary_file, "w", encoding="utf-8") as f:
        for email in parsed_emails:
            f.write(f"\n--- Email #{email['Email Sequence']} ---\n")
            f.write(f"From: {email['Sender']}\n")
            f.write(f"To: {email['Receiver']}\n")
            f.write(f"CC: {email['cc']}\n")
            f.write(f"Subject: {email['subject']}\n")
            f.write(f"Date: {email['date']} {email['time']}\n")
            f.write("Body:\n")
            f.write(email["email body"] + "\n")
            f.write("Tokens:\n")
            f.write(email["tokens"] + "\n")
    print(f"‚úÖ Summary saved: {summary_file}")

approval_keywords = [
    "approved", "accepted", "authorized", "endorsed", "sanctioned", "confirmed",
    "looks good", "go ahead", "proceed", "continue", "carry on", "move forward",
    "approve", "please proceed", "okay with me", "works for me", "noted", "acknowledged",
    "approved by me", "authorized by me"
]

request_keywords = [
    "please approve", "can you approve", "requesting approval", "need your approval",
    "seeking approval", "approval needed", "kindly approve", "requesting your review",
    "for your review", "approval request"
]

def identify_requester_approver(parsed_emails):
    approver = ""
    requester = ""

    for email in parsed_emails:
        token_text = email["tokens"].lower()
        if any(k in token_text for k in approval_keywords):
            approver = email["Sender"]
        if any(k in token_text for k in request_keywords):
            requester = email["Sender"]

    if not requester:
        for email in reversed(parsed_emails):
            if email["Sender"] != approver:
                requester = email["Sender"]
                break

    return requester, approver

def save_roles(requester, approver, output_file="requester_approver.txt"):
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"Requester: {requester}\n")
        f.write(f"Approver: {approver}\n")
    print(f"‚úÖ Roles saved: {output_file}")

def main():
    if len(sys.argv) != 2:
        print("Usage: python email_parser.py <email_chain.txt>")
        return

    filepath = sys.argv[1]
    if not os.path.exists(filepath):
        print(f"‚ùå File not found: {filepath}")
        return

    with open(filepath, "r", encoding="utf-8", errors="replace") as f:
        text = f.read()

    parsed_emails = parse_email_chain(text)
    print(f"üì© Parsed {len(parsed_emails)} emails in the chain.")
    print("üß† Analyzing tokens and detecting roles...")

    requester, approver = identify_requester_approver(parsed_emails)

    print("\nüîç Identified Roles:")
    print(f"Requester: {requester}")
    print(f"Approver: {approver}")

    save_to_csv(parsed_emails)
    save_summary(parsed_emails)
    save_roles(requester, approver)

if __name__ == "__main__":
    main()
