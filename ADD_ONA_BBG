from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as sum_

# Create a Spark session (Dataiku handles this automatically in PySpark)
spark = SparkSession.builder.appName("DataikuProcessing").getOrCreate()

# Read input dataset from Dataiku
input_dataset = dataiku.Dataset("your_input_dataset")  # Change to your dataset name
df = input_dataset.get_dataframe(spark)  # Load as PySpark DataFrame

# Fill null values in 'amount', convert to double, and filter values >= 0
processed_df = df.fillna(0, subset=["amount"]) \
                 .withColumn("amount", col("amount").cast("double")) \
                 .filter(col("amount") >= 0)

# Group by 'category' and calculate sum of 'amount'
grouped_df = processed_df.groupBy("category").agg(sum_("amount").alias("total_amount"))

# Write output to Dataiku dataset
output_dataset = dataiku.Dataset("your_output_dataset")  # Change to your output dataset name
output_dataset.write_with_schema(grouped_df)

# Save output to a CSV file
output_path = "/home/dataiku/output/grouped_data.csv"  # Adjust path as needed
grouped_df.write.csv(output_path, header=True)
