import numpy as np
np.int = int
np.float = float

import nltk
import re
import csv
import sys
import os
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from dateutil.parser import parse as parse_date

# Setup NLTK path (optional: update as needed)
nltk.data.path.append(r'C:\Users\2011747\nltk_library')

# Ensure stopwords are available
try:
    stop_words = set(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')
    stop_words = set(stopwords.words('english'))

def split_emails(raw_text):
    parts = re.split(r"(?=^From: )", raw_text, flags=re.IGNORECASE | re.MULTILINE)
    if not parts[0].strip().lower().startswith("from:"):
        first = parts.pop(0)
        parts = [first] + parts
    return parts

def extract_field(email, field):
    pattern = rf"{field}:(.*?)(?=\n\S+:|\Z)"
    match = re.search(pattern, email, re.IGNORECASE | re.DOTALL)
    return match.group(1).strip().replace('\n', ' ') if match else ""

def parse_date_time(date_str):
    if not date_str:
        return "", ""
    try:
        dt = parse_date(date_str.strip(), fuzzy=True)
        return dt.date().isoformat(), dt.time().isoformat()
    except:
        return date_str.strip(), ""

def extract_body(email):
    split_point = re.search(r"\n\s*\n", email)
    if split_point:
        return email[split_point.end():].strip()
    return email.strip()

def tokenize_body(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    words = word_tokenize(text)
    cleaned = [w for w in words if w not in stop_words and w.isalpha()]
    return cleaned

def parse_email_chain(text):
    email_chunks = split_emails(text)
    parsed = []
    for i, email in enumerate(email_chunks):
        sender = extract_field(email, "From")
        receiver = extract_field(email, "To")
        cc = extract_field(email, "Cc")
        bcc = extract_field(email, "Bcc")
        subject = extract_field(email, "Subject")
        date_raw = extract_field(email, "Sent")

        date, time = parse_date_time(date_raw)
        body = extract_body(email)
        tokens = tokenize_body(body)

        parsed.append({
            "Email Sequence": i + 1,
            "Sender": sender,
            "Receiver": receiver,
            "cc": cc,
            "bcc": bcc,
            "subject": subject,
            "email body": body,
            "tokens": ", ".join(tokens),
            "date": date,
            "time": time
        })
    return parsed

def save_to_csv(parsed_emails, output_file="parsed_emails.csv"):
    if not parsed_emails:
        print("❌ No emails found.")
        return
    fieldnames = parsed_emails[0].keys()
    with open(output_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in parsed_emails:
            writer.writerow(row)
    print(f"✅ CSV saved: {output_file}")

def save_summary(parsed_emails, summary_file="summary_output.txt"):
    with open(summary_file, "w", encoding="utf-8") as f:
        for email in parsed_emails:
            f.write(f"\n--- Email #{email['Email Sequence']} ---\n")
            f.write(f"From: {email['Sender']}\n")
            f.write(f"To: {email['Receiver']}\n")
            f.write(f"CC: {email['cc']}\n")
            f.write(f"Subject: {email['subject']}\n")
            f.write(f"Date: {email['date']} {email['time']}\n")
            f.write("Body:\n")
            f.write(email["email body"] + "\n")
            f.write("Tokens:\n")
            f.write(email["tokens"] + "\n")
    print(f"✅ Summary saved: {summary_file}")

def identify_requester_approver(parsed_emails):
    if len(parsed_emails) >= 2:
        requester = parsed_emails[0]["Sender"]
        approver = parsed_emails[1]["Sender"]
    elif len(parsed_emails) == 1:
        requester = parsed_emails[0]["Sender"]
        approver = ""
    else:
        requester = approver = ""
    return requester, approver

def save_roles(requester, approver, output_file="requester_approver.txt"):
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"Requester: {requester}\n")
        f.write(f"Approver: {approver}\n")
    print(f"✅ Roles saved: {output_file}")

def main():
    if len(sys.argv) != 2:
        print("Usage: python email_parser.py <email_chain.txt>")
        return

    filepath = sys.argv[1]
    if not os.path.exists(filepath):
        print(f"❌ File not found: {filepath}")
        return

    with open(filepath, "r", encoding="utf-8", errors="replace") as f:
        text = f.read()

    parsed_emails = parse_email_chain(text)

    # Add datetime object to each email
    for email in parsed_emails:
        try:
            email['datetime_obj'] = datetime.strptime(f"{email['date']} {email['time']}", "%Y-%m-%d %H:%M:%S")
        except:
            email['datetime_obj'] = datetime.max  # fallback if date parsing fails

    # Sort emails chronologically
    parsed_emails.sort(key=lambda x: x['datetime_obj'])

    print(f"📩 Parsed {len(parsed_emails)} emails in the chain.")
    print("🧠 Identifying requester and approver based on chronological order...")

    requester, approver = identify_requester_approver(parsed_emails)

    print("\n🔍 Identified Roles:")
    print(f"Requester: {requester}")
    print(f"Approver: {approver}")

    save_to_csv(parsed_emails)
    save_summary(parsed_emails)
    save_roles(requester, approver)

if __name__ == "__main__":
    main()
