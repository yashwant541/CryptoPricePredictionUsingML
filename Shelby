import os
import re
import sys
import csv
import math
import shutil
import tempfile
from datetime import datetime
import dataiku
from fuzzywuzzy import fuzz

# -----------------------------
# Enhanced Configuration
# -----------------------------
class Config:
    SCORE_WEIGHTS = {
        'keyword_presence': 25,
        'semantic_patterns': 30,
        'temporal_position': 20,
        'conversation_flow': 15,
        'hierarchy_analysis': 10
    }
    
    CONFIDENCE_THRESHOLDS = {
        'very_high': 0.5,
        'high': 0.3,
        'medium': 0.2,
        'low': 0.0
    }

# Enhanced keyword dictionaries with weights
APPROVER_KEYWORDS = {
    "approved": 100, "granted": 95, "confirmed": 90, "accepted": 90,
    "authorized": 95, "approve": 80, "confirm": 80, "accept": 80,
    "approval": 50, "clearance": 70, "endorsed": 75, "signed off": 85,
    "cleared": 80, "validated": 85, "ratified": 90, "sanctioned": 90,
    "permission granted": 95, "fully supported": 85, "completely endorse": 80
}

REQUESTER_KEYWORDS = {
    "request": 100, "require": 95, "seek approval": 90, "need approval": 90,
    "asking": 80, "petition": 75, "approval": 30, "pending": 60,
    "remind": 70, "follow up": 65, "submitted": 85, "application": 80,
    "awaiting": 75, "seeking": 85, "would like to request": 95,
    "please approve": 90, "kindly approve": 90, "require authorization": 85
}

# -----------------------------
# Enhanced Helper Functions
# -----------------------------
def log(message):
    print(f"[LOG] {message}", file=sys.stderr, flush=True)

def split_emails(raw_text):
    parts = re.split(r"(?=^From: )", raw_text, flags=re.IGNORECASE | re.MULTILINE)
    if parts and not parts[0].strip().lower().startswith("from:"):
        first = parts.pop(0)
        parts = [first] + parts
    return parts

def extract_field(email, field):
    pattern = rf"{field}:(.*)"
    match = re.search(pattern, email, re.IGNORECASE)
    return match.group(1).strip() if match else ""

def parse_date_time(date_str):
    if not date_str:
        return None
    try:
        return datetime.strptime(date_str.strip(), "%A, %B %d, %Y %I:%M %p")
    except Exception:
        return None

def extract_body(email):
    split_point = re.search(r"\n\s*\n", email)
    return email[split_point.end():].strip() if split_point else ""

def clean_participant_name(raw_name):
    if not raw_name:
        return ""
    cleaned = re.sub(r'<[^>]+>', '', raw_name)
    cleaned = re.sub(r'[;"\']', '', cleaned)
    cleaned = ' '.join(cleaned.split()).strip()
    cleaned = cleaned.rstrip(',')
    return cleaned

def find_matching_statement(email_body, keywords, threshold=80):
    sentences = re.split(r'(?<=[.!?])\s+', email_body.strip())
    exact_matches, fuzzy_matches = [], []
    for sentence in sentences:
        clean_sentence = sentence.lower()
        for kw in keywords:
            if kw in clean_sentence:
                exact_matches.append(sentence.strip())
            else:
                ratio = fuzz.partial_ratio(kw, clean_sentence)
                if ratio >= threshold:
                    fuzzy_matches.append((sentence.strip(), kw, ratio))
    if exact_matches:
        return "; ".join(exact_matches), "exact"
    elif fuzzy_matches:
        best_match = max(fuzzy_matches, key=lambda x: x[2])
        return best_match[0], "fuzzy"
    return "", ""

# -----------------------------
# Enhanced Email Parsing
# -----------------------------
def parse_email_chain(text):
    log("Starting email chain parsing...")
    email_chunks = split_emails(text)
    log(f"Found {len(email_chunks)} email chunks")
    
    parsed = []
    for i, email in enumerate(email_chunks):
        try:
            sender = extract_field(email, "From")
            receiver = extract_field(email, "To")
            cc = extract_field(email, "Cc")
            bcc = extract_field(email, "Bcc")
            subject = extract_field(email, "Subject")
            date_raw = extract_field(email, "Sent")
            dt = parse_date_time(date_raw)
            date_str = dt.date().isoformat() if dt else ""
            time_str = dt.time().isoformat() if dt else ""
            body = extract_body(email)
            
            # Ensure we always have these fields, even if empty
            approval_statement, approval_type = find_matching_statement(body, APPROVER_KEYWORDS.keys())
            request_statement, request_type = find_matching_statement(body, REQUESTER_KEYWORDS.keys())
            
            # Extract semantic patterns
            semantic_patterns = extract_semantic_patterns(body)
            
            parsed.append({
                "Email Sequence": i + 1,
                "Sender": sender,
                "Receiver": receiver,
                "cc": cc,
                "bcc": bcc,
                "subject": subject,
                "email body": body,
                "approval statement": approval_statement or "",
                "approval match type": approval_type or "",
                "request statement": request_statement or "",
                "request match type": request_type or "",
                "semantic_patterns": str(semantic_patterns),
                "datetime": dt,
                "date": date_str,
                "time": time_str
            })
            log(f"‚úÖ Parsed email {i+1}: {sender} -> {receiver}")
        except Exception as e:
            log(f"‚ö†Ô∏è Error parsing email {i+1}: {str(e)}")
            continue
    
    # Sort by datetime
    parsed = sorted(parsed, key=lambda x: x["datetime"] if x["datetime"] else datetime.min)
    for i, email in enumerate(parsed):
        email["Email Sequence"] = i + 1
    
    log(f"‚úÖ Successfully parsed {len(parsed)} emails")
    return parsed

def extract_all_participants(parsed_emails):
    participants = set()
    for email in parsed_emails:
        sender = clean_participant_name(email.get("Sender", ""))
        if sender: participants.add(sender)
        
        receiver_field = email.get("Receiver", "")
        receivers = [clean_participant_name(r) for r in receiver_field.split(',')] if receiver_field else []
        participants.update([r for r in receivers if r])
        
        cc_field = email.get("cc", "")
        cc = [clean_participant_name(c) for c in cc_field.split(',')] if cc_field else []
        participants.update([c for c in cc if c])
        
        bcc_field = email.get("bcc", "")
        bcc = [clean_participant_name(b) for b in bcc_field.split(',')] if bcc_field else []
        participants.update([b for b in bcc if b])
    
    log(f"üìß Found {len(participants)} participants: {list(participants)}")
    return sorted(participants)

# -----------------------------
# Enhanced Linguistic Analysis
# -----------------------------
def extract_semantic_patterns(email_body):
    """Extract more sophisticated linguistic patterns"""
    patterns = {
        "explicit_approval": [
            r"(?:I\s+)?(?:hereby\s+)?approve(?:\s+the\s+request)?",
            r"(?:request|application)\s+(?:is\s+)?approved",
            r"grant(?:ed|ing)\s+(?:the\s+)?(?:request|permission)",
            r"fully\s+supported|completely\s+endorse"
        ],
        "conditional_approval": [
            r"approved\s+subject\s+to|conditional\s+approval",
            r"pending\s+[^.]*approval"
        ],
        "explicit_request": [
            r"(?:I\s+)?(?:would\s+like\s+to\s+)?request(?:\s+approval)?",
            r"seeking\s+(?:your\s+)?approval",
            r"please\s+approve|kindly\s+approve",
            r"require\s+(?:your\s+)?authorization"
        ],
        "delegated_authority": [
            r"on\s+behalf\s+of|acting\s+for",
            r"delegated\s+authority"
        ]
    }
    
    found_patterns = {}
    for pattern_type, regex_list in patterns.items():
        for regex in regex_list:
            if re.search(regex, email_body, re.IGNORECASE):
                found_patterns[pattern_type] = found_patterns.get(pattern_type, 0) + 1
    
    return found_patterns

# -----------------------------
# Enhanced Scoring Engine
# -----------------------------
class AdvancedScoringEngine:
    def __init__(self):
        self.weights = Config.SCORE_WEIGHTS
    
    def _calculate_keyword_scores(self, parsed_emails):
        """Calculate scores based on keyword presence"""
        participants = extract_all_participants(parsed_emails)
        scores = {p: {"maker": 0, "checker": 0} for p in participants}
        
        for email in parsed_emails:
            sender = clean_participant_name(email.get("Sender", ""))
            if not sender: continue
            
            # Approval keywords - with safe access
            approval_statement = email.get("approval_statement", "")
            if approval_statement:
                for keyword, weight in APPROVER_KEYWORDS.items():
                    if keyword in approval_statement.lower():
                        scores[sender]["checker"] += weight
                        break
            
            # Request keywords - with safe access
            request_statement = email.get("request_statement", "")
            if request_statement:
                for keyword, weight in REQUESTER_KEYWORDS.items():
                    if keyword in request_statement.lower():
                        scores[sender]["maker"] += weight
                        break
        
        log(f"üîë Keyword scores calculated for {len(scores)} participants")
        return scores
    
    def calculate_comprehensive_scores(self, parsed_emails):
        participants = extract_all_participants(parsed_emails)
        scores = {p: {"maker": 0, "checker": 0, "breakdown": {}} for p in participants}
        
        # For single email, use simple logic
        if len(parsed_emails) == 1:
            log("üîÑ Single email detected, using simplified scoring")
            email = parsed_emails[0]
            sender = clean_participant_name(email.get("Sender", ""))
            
            if sender in scores:
                # Check for approval keywords
                approval_statement = email.get("approval_statement", "")
                if approval_statement:
                    scores[sender]["checker"] = 100
                    log(f"‚úÖ {sender} identified as Checker (approval found)")
                
                # Check for request keywords  
                request_statement = email.get("request_statement", "")
                if request_statement:
                    scores[sender]["maker"] = 100
                    log(f"‚úÖ {sender} identified as Maker (request found)")
                
                # If no clear keywords, first sender is maker
                if not approval_statement and not request_statement:
                    scores[sender]["maker"] = 80
                    log(f"‚ö†Ô∏è No clear keywords, defaulting {sender} as Maker")
        else:
            # Multi-email logic (simplified for debugging)
            log("üîÑ Multiple emails detected, using multi-email scoring")
            keyword_scores = self._calculate_keyword_scores(parsed_emails)
            
            for participant in participants:
                scores[participant]["maker"] = keyword_scores[participant]["maker"]
                scores[participant]["checker"] = keyword_scores[participant]["checker"]
        
        log(f"üìä Final scores: {scores}")
        return scores

# -----------------------------
# Enhanced Maker-Checker Identification
# -----------------------------
def identify_maker_checker(parsed_emails):
    log("üéØ Starting maker-checker identification...")
    
    if not parsed_emails: 
        log("‚ùå No emails to analyze")
        return "", "", {}, "Very Low"
    
    # Use simplified scoring engine
    scoring_engine = AdvancedScoringEngine()
    scores = scoring_engine.calculate_comprehensive_scores(parsed_emails)
    
    # Find maker (requester) - highest maker score
    maker_candidates = [(p, s["maker"]) for p, s in scores.items() if s["maker"] > 0]
    checker_candidates = [(p, s["checker"]) for p, s in scores.items() if s["checker"] > 0]
    
    maker_candidates.sort(key=lambda x: x[1], reverse=True)
    checker_candidates.sort(key=lambda x: x[1], reverse=True)
    
    # Get identification
    maker = maker_candidates[0][0] if maker_candidates else ""
    checker = checker_candidates[0][0] if checker_candidates else ""
    
    # Simple confidence calculation
    if maker and checker:
        confidence = "High"
    elif maker or checker:
        confidence = "Medium"
    else:
        confidence = "Low"
    
    log(f"‚úÖ Identification complete - Maker: {maker}, Checker: {checker}, Confidence: {confidence}")
    return maker, checker, scores, confidence

# -----------------------------
# Enhanced Dataiku Folder Save/Load
# -----------------------------
def save_enhanced_csv(parsed_emails, output_folder, filename):
    try:
        log(f"üíæ Attempting to save CSV: {filename}")
        output_path = os.path.join(tempfile.gettempdir(), filename)
        
        fieldnames = [
            "Email Sequence", "Sender", "Receiver", "cc", "bcc", "subject", 
            "email body", "approval statement", "approval match type", 
            "request statement", "request match type", "semantic_patterns",
            "datetime", "date", "time"
        ]
        
        with open(output_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for row in parsed_emails:
                writer.writerow(row)
        
        log(f"üìÅ CSV created at {output_path}, size: {os.path.getsize(output_path)} bytes")
        
        with open(output_path, "rb") as f:
            output_folder.upload_stream(filename, f)
        
        os.remove(output_path)
        log(f"‚úÖ Successfully saved {filename} to Dataiku")
        return True
        
    except Exception as e:
        log(f"‚ùå Error saving CSV {filename}: {str(e)}")
        return False

def save_simple_summary(parsed_emails, maker, checker, confidence, output_folder, filename):
    try:
        log(f"üíæ Saving summary: {filename}")
        output_path = os.path.join(tempfile.gettempdir(), filename)
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("=== MAKER-CHECKER ANALYSIS ===\n\n")
            f.write(f"Maker (Requester): {maker}\n")
            f.write(f"Checker (Approver): {checker}\n")
            f.write(f"Confidence Level: {confidence}\n")
            f.write(f"Total Emails Analyzed: {len(parsed_emails)}\n\n")
            
            f.write("=== PARTICIPANTS ===\n")
            participants = extract_all_participants(parsed_emails)
            for participant in participants:
                f.write(f"- {participant}\n")
        
        with open(output_path, "rb") as f:
            output_folder.upload_stream(filename, f)
        
        os.remove(output_path)
        log(f"‚úÖ Successfully saved summary: {filename}")
        return True
        
    except Exception as e:
        log(f"‚ùå Error saving summary {filename}: {str(e)}")
        return False

# -----------------------------
# SIMPLIFIED Main Execution
# -----------------------------
def main():
    log("üöÄ STARTING MAKER-CHECKER ANALYSIS")
    
    # Replace these with your actual Dataiku folder codes
    INPUT_FOLDER_CODE = "YOUR_INPUT_FOLDER_ID"   
    OUTPUT_FOLDER_CODE = "YOUR_OUTPUT_FOLDER_ID"
    
    log(f"üìÅ Input folder: {INPUT_FOLDER_CODE}")
    log(f"üìÅ Output folder: {OUTPUT_FOLDER_CODE}")

    try:
        input_folder = dataiku.Folder(INPUT_FOLDER_CODE)
        output_folder = dataiku.Folder(OUTPUT_FOLDER_CODE)
        log("‚úÖ Successfully connected to Dataiku folders")
    except Exception as e:
        log(f"‚ùå Error connecting to Dataiku: {str(e)}")
        return

    # List all files in input folder
    try:
        all_files = input_folder.list_paths_in_partition()
        log(f"üìÇ Found {len(all_files)} files in input folder: {all_files}")
    except Exception as e:
        log(f"‚ùå Error listing files: {str(e)}")
        return

    if not all_files:
        log("‚ùå No files found in input folder")
        return

    processed_count = 0
    
    for file_path in all_files:
        log(f"\nüîç Processing file: {file_path}")
        
        try:
            # Download file to temporary location
            with tempfile.NamedTemporaryFile(suffix=".txt", delete=False, mode='wb') as tmp_file:
                tmp_path = tmp_file.name
                log(f"üíæ Downloading to temporary file: {tmp_path}")
                
                with input_folder.get_download_stream(file_path) as stream:
                    shutil.copyfileobj(stream, tmp_file)
                
                log(f"‚úÖ Downloaded {os.path.getsize(tmp_path)} bytes")

            # Read file content
            with open(tmp_path, "r", encoding="utf-8", errors='ignore') as f:
                email_text = f.read()
            
            log(f"üìñ Read {len(email_text)} characters from file")
            
            # Parse emails
            parsed_emails = parse_email_chain(email_text)
            
            if not parsed_emails:
                log(f"‚ö†Ô∏è No emails parsed from {file_path}")
                os.remove(tmp_path)
                continue

            # Identify maker and checker
            maker, checker, scores, confidence = identify_maker_checker(parsed_emails)
            
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            log(f"üìù Base name for outputs: {base_name}")

            # Save outputs
            csv_success = save_enhanced_csv(parsed_emails, output_folder, f"parsed_emails_{base_name}.csv")
            summary_success = save_simple_summary(parsed_emails, maker, checker, confidence, output_folder, f"summary_{base_name}.txt")
            
            if csv_success and summary_success:
                processed_count += 1
                log(f"üéâ Successfully processed {file_path}")
            else:
                log(f"‚ö†Ô∏è Some outputs failed for {file_path}")

            # Clean up
            os.remove(tmp_path)
            log(f"üßπ Cleaned up temporary files for {file_path}")

        except Exception as e:
            log(f"‚ùå Error processing file {file_path}: {str(e)}")
            import traceback
            log(f"‚ùå Traceback: {traceback.format_exc()}")

    log(f"\nüéä PROCESSING COMPLETE")
    log(f"üìä Successfully processed {processed_count} out of {len(all_files)} files")

    # List output files to verify
    try:
        output_files = output_folder.list_paths_in_partition()
        log(f"üìÇ Output folder now contains {len(output_files)} files: {output_files}")
    except Exception as e:
        log(f"‚ö†Ô∏è Could not list output files: {str(e)}")

if __name__ == "__main__":
    main()
